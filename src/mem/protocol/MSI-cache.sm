/*
 * Copyright (c) 1999-2013 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OtherWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*/

machine(L1Cache, "MSI Snooping L1 Cache CMP")
 : Sequencer * sequencer;
   CacheMemory * Icache;
   CacheMemory * Dcache;
   Cycles l1_request_latency := 5;
   Cycles l1_response_latency := 2;
   bool send_evictions;
	 bool is_blocked;

   // Message Queues

	 // To the network
	 // requestFromCache: Cache controller will receive messages from the cache and propogate to the bus
	 MessageBuffer * requestFromCache, network="To", virtual_network="2", ordered="false", vnet_type="request";
	 // responseFromCache: Cache controller will respond to cache activity from other cores
	 MessageBuffer * responseFromCache, network="To", virtual_network="4", ordered="false", vnet_type="response";

	 // Atomic ack vnet
	 MessageBuffer * atomicRequestFromCache, network="To", virtual_network="5", ordered="false", vnet_type="atomicRequest";

	 // From the network
	 // responseToCache: Cache controller will recieve replies based on the requests it issued on responeFromCache network
	 MessageBuffer * responseToCache, network="From", virtual_network="4", ordered="false", vnet_type="response";
	 // requestToCache: Cache controller will recieve replies based on other cache controller activities
	 MessageBuffer * requestToCache, network="From", virtual_network="2", ordered="false", vnet_type="request";

	// Atomic ack vnet
	MessageBuffer * atomicRequestToCache, network="From", virtual_network="5", ordered="false", vnet_type="atomicRequest";
{
  // STATES
  state_declaration(State, desc="Cache states", default="L1Cache_State_I") {
    // Base states
    NP, AccessPermission:Invalid, desc="Not present in either cache";
    I, AccessPermission:Invalid, desc="a L1 cache entry Idle";
    S, AccessPermission:Read_Only, desc="a L1 cache entry Shared";
    M, AccessPermission:Read_Write, desc="a L1 cache entry Modified", format="!b";


    // Transient States
    IS_AD, AccessPermission:Busy, desc="L1 idle, issued a load, have not seen response yet";
    IS_D, AccessPermission:Busy, desc="L1 idle, issued a load, saw own GETS, not seen data response yet";
    IS_DI, AccessPermission:Busy, desc="L1 idle, issued a load, saw own GETS, then saw remote GETM not seen data response yet";
    IS_A, AccessPermission:Busy, desc="L1 idle, issued a load, saw own GETS, not seen data response yet";
		IM_AD, AccessPermission:Busy, desc="L1 idle, issued GETX, have not seen response yet";

		IM_D, AccessPermission:Busy, desc="L1 idle, issued GETX, saw own GETX, not seen data response yet";
		IM_A, AccessPermission:Busy, desc="lisnfd";
		IM_DS, AccessPermission:Busy, desc="lisnfd";
		IM_DI, AccessPermission:Busy, desc="lisnfd";
		IM_DSI, AccessPermission:Busy, desc="lisnfd";
    SM, AccessPermission:Read_Only, desc="L1 read-only, issued GETX, have not seen response yet";
    IS_I, AccessPermission:Busy, desc="L1 idle, issued GETS, saw another GETX from a remote core";
		IM_S, AccessPermission:Busy, desc="L1 idle, issued GetX, saw another GETS from a remote core";
		IM_SI, AccessPermission:Busy, desc="L1 idle, issued GetX, saw another GETS from a remote core, followed by a GETX from a remote core";
		SM_I, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote GETX. This core has to send data to remote GetX core and invalidate"; 
		SM_S, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS. This core has to send the data to GetS remote core and move to S";
		SM_SI, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
		SM_AD, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
		SM_A, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
		SM_D, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
		SM_DS, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
		SM_DSI, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
		SM_DI, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
    M_I, AccessPermission:Read_Only, desc="L1 replacing, waiting for ACK. Serive remote load and store misses";
    MI_A, AccessPermission:Read_Only, desc="L1 replacing, waiting for ACK. Serive remote load and store misses";
    MS_A, AccessPermission:Read_Only, desc="L1 replacing, waiting for ACK. Serive remote load and store misses";
    II_A, AccessPermission:Read_Only, desc="L1 replacing, waiting for ACK. Serive remote load and store misses";
		I_I, AccessPermission:Busy, desc="Invalid, WB occuring";

  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
		
		// Processor generated events
		Load, desc="Load request from processor";
		Store, desc="Store request from processor";
		Ifetch, desc="Instruction fetch request from processor";
		RMW_Read, desc="Read-modify-write read from processor";
		RMW_Write, desc="Read-modify-write write from processor";
		Replacement, desc="Replacement from processor";
		Load_Approx, desc="Load approx request from processor";
		Store_Approx, desc="Store approx request from processor";

		// Requests observed on the bus
		Other_GETS, desc="Remote processor doing load";
		Other_GETM, desc="Remote processor doing store";
		Other_GETI, desc="Remote processor doing instruction fetch";
		Other_PUTM, desc="Remote processor doing a PUTM";
		OWN_GETS, desc="Processor sees its own GETS coherence message ordered on the network";
		OWN_GETM, desc="Processor sees its own GETM coherence message ordered on the network";
		OWN_GETI, desc="Procssor sees its own GETI coherence message ordered on the network";
		OWN_PUTM, desc="Processor sees its own PUTM";
		INV, desc="Processor sees INV signal from Mem";

		// Atomic requests
		ATOMICST, desc="Atomic request";
		ATOMICEN, desc="Atomic request";

		// Data observed on the bus
		Data, desc="Data sent by shared mem. No cache-to-cache transfer";

	  // Acks
		Ack, desc="Ack for processor";
  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry" ) {
    State CacheState,        desc="cache state";
    DataBlock DataBlk,       desc="data for the block";
    bool Dirty, default="false",   desc="data is dirty";
		// No unsigned int
		MachineID finalDestination, desc="where this cache block should go in cache-to-cache transfer";
		NetDest finalDestinationSet, desc="where this cache block should go in cache-to-cache transfer";
		// Is this an atomic request?
		bool isAtomic, default="false", desc="data is atomic";
  }

  // TBE fields
  structure(TBE, desc="...") {
    Address Addr,              desc="Physical address for this TBE";
    State TBEState,        desc="Transient state";
    DataBlock DataBlk,                desc="Buffer for the data block";
    bool Dirty, default="false",   desc="data is dirty"; 
    int pendingAcks, default="0", desc="number of pending acks";
		bool isAtomic, default="false", desc="data is atomic";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }

  TBETable TBEs, template="<L1Cache_TBE>", constructor="m_number_of_TBEs";

  MessageBuffer mandatoryQueue, ordered="false";

  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Address a);
	void wakeUpAllBuffers();
  void profileMsgDelay(int virtualNetworkType, Cycles c);

  // inclusive cache returns L1 entries only
  Entry getL1CacheEntry(Address addr), return_by_pointer="yes" {
		Entry Dcache_entry := static_cast(Entry, "pointer", Dcache[addr]);
    if(is_valid(Dcache_entry)) {
      return Dcache_entry;
    }

    Entry Icache_entry := static_cast(Entry, "pointer", Icache[addr]);
    return Icache_entry;
  }

	bool isAtomic(Address addr) {
	  //TBE tbe := TBEs[addr];
		//if (is_valid(tbe)) {
		//	return tbe.isAtomic;
		//}

    Entry cache_entry := getL1CacheEntry(addr);
		if (is_valid(cache_entry)) {
			return cache_entry.isAtomic;
		}
		return false;
	}

  Entry getDCacheEntry(Address addr), return_by_pointer="yes" {
    Entry Dcache_entry := static_cast(Entry, "pointer", Dcache[addr]);
    return Dcache_entry;
  }

  Entry getICacheEntry(Address addr), return_by_pointer="yes" {
    Entry Icache_entry := static_cast(Entry, "pointer", Icache[addr]);
    return Icache_entry;
  }

  State getState(TBE tbe, Entry cache_entry, Address addr) {

    assert((Dcache.isTagPresent(addr) && Icache.isTagPresent(addr)) == false);

    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:I;
  }

	bool is_dataSendState(TBE tbe, Address addr, Entry cache_entry) {	
		State returnState := getState(tbe, cache_entry, addr);
		if (returnState == State:M || returnState == State:IM_D || returnState == State:SM_D || returnState == State:MI_A) {
			return true;
		}
		return false;
	}

	bool is_dataWaitingState(TBE tbe, Address addr, Entry cache_entry) {
		State returnState := getState(tbe, cache_entry, addr);
		// This function is for atomic operations
		if (returnState == State:IM_D || returnState == State:SM_D || returnState == State:M) {
			return true;
		}
		return false;
	}

	bool is_atomicInvariant(TBE tbe, Address addr, Entry cache_entry) {
		State returnState := getState(tbe, cache_entry, addr);
		if (returnState == State:M || returnState == State:S || returnState == State:I) {
			return false;
		}
		return true;
	}

  void setState(TBE tbe, Entry cache_entry, Address addr, State state) {

    assert((Dcache.isTagPresent(addr) && Icache.isTagPresent(addr)) == false);

    // MUST CHANGE
    if(is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }

  }

  AccessPermission getAccessPermission(Address addr) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s addr: %s\n", L1Cache_State_to_permission(tbe.TBEState), addr);
      return L1Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getL1CacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s addr: %s\n", L1Cache_State_to_permission(cache_entry.CacheState), addr);
      return L1Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s addr: %s\n", AccessPermission:NotPresent, addr);
    return AccessPermission:NotPresent;
  }

  void functionalRead(Address addr, Packet *pkt) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      testAndRead(addr, tbe.DataBlk, pkt);
    } else {
      testAndRead(addr, getL1CacheEntry(addr).DataBlk, pkt);
    }
  }

  int functionalWrite(Address addr, Packet *pkt) {
    int num_functional_writes := 0;

    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      num_functional_writes := num_functional_writes +
        testAndWrite(addr, tbe.DataBlk, pkt);
      return num_functional_writes;
    }

    num_functional_writes := num_functional_writes +
        testAndWrite(addr, getL1CacheEntry(addr).DataBlk, pkt);
    return num_functional_writes;
  }

  void setAccessPermission(Entry cache_entry, Address addr, State state) {

    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L1Cache_State_to_permission(state));
    }

  }

  Event mandatory_request_type_to_event(RubyRequestType type) {
    if (type == RubyRequestType:LD) {
      ++sequencer.loads;
      return Event:Load;
    } 
		else if (type == RubyRequestType:LD_APPROX) {
			++sequencer.loads;
			++sequencer.approx_loads;
			return Event:Load_Approx;
		}
		else if (type == RubyRequestType:IFETCH) {
      ++sequencer.ifetch;
      return Event:Ifetch;
      //return Event:Load;
    } else if ((type == RubyRequestType:ST) || (type == RubyRequestType:ATOMIC)) {
      ++sequencer.stores;
      return Event:Store;
    }
		else if (type == RubyRequestType:ST_APPROX) {
			++sequencer.stores;
			++sequencer.approx_stores;
			// Record difference in data
			return Event:Store_Approx;
		}
		else if (type == RubyRequestType:Locked_RMW_Read) {
			// RMW_Write should finally transition to an M_ATOMIC state that RMW_Write will pick up from
			return Event:RMW_Read;
			++sequencer.stores;
		}
		else if (type == RubyRequestType:Locked_RMW_Write) {
			// RMW_Write should finally transition to an M state
			return Event:RMW_Write;
			++sequencer.stores;
		}
		
		else {
      error("Invalid RubyRequestType");
    }
  }


  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }


	// ** OUT_PORTS **
  out_port(requestNetwork_out, RequestMsg, requestFromCache);
  out_port(responseNetwork_out, ResponseMsg, responseFromCache);
	out_port(atomicRequestNetwork_out, ResponseMsg, atomicRequestFromCache);

	in_port(atomicRequestNetwork_in, RequestMsg, atomicRequestToCache, rank=1) {
		if (atomicRequestNetwork_in.isReady()) {
			peek(atomicRequestNetwork_in, RequestMsg, block_on="Addr") {
				Entry cache_entry := getL1CacheEntry(in_msg.Addr);
				TBE tbe := TBEs[in_msg.Addr];

				if (in_msg.Type == CoherenceRequestType:GETMATOMICEN) {
					if (in_msg.Requestor != machineID) {
						DPRINTF(RubySlicc, "ATOMIC EN request issued on address: %s\n", in_msg.Addr);
						trigger(Event:ATOMICEN, in_msg.Addr, cache_entry, tbe);
					}
					else {
						atomicRequestNetwork_in.dequeue();
					}
				}
				else if (in_msg.Type == CoherenceRequestType:GETMATOMICST) {
					if (in_msg.Requestor != machineID) {
						DPRINTF(RubySlicc, "ATOMIC request issued on address: %s\n", in_msg.Addr);
						trigger(Event:ATOMICST, in_msg.Addr, cache_entry, tbe);
					}
					else {
						atomicRequestNetwork_in.dequeue();
					}
				}
			}
		}
	}
	
	
	// Response to cache
	in_port(responseNetwork_in, ResponseMsg, responseToCache, rank=1) {
		if (responseNetwork_in.isReady()) {
			//if (is_blocked == false) {
				peek(responseNetwork_in, ResponseMsg, block_on="Addr") {
					// Making sure directory is sending to data to the right guy

      	  DPRINTF(RubySlicc, "Response address:%s \n",
                in_msg.Addr);


					//assert(in_msg.Destination.isElement(machineID));
	
					Entry cache_entry := getL1CacheEntry(in_msg.Addr);
					TBE tbe := TBEs[in_msg.Addr];


					// Check if its data
					if (in_msg.Type == CoherenceResponseType:DATA) {
						DPRINTF(RubySlicc, "DATA SENT TYPE FROM DIR\n ");
						trigger(Event:Data, in_msg.Addr, cache_entry, tbe);
					}	
					if (in_msg.Type == CoherenceResponseType:DATA_TO_CACHE) {
						DPRINTF(RubySlicc, "DATA SENT TYPE FROM CACHE\n");
						trigger(Event:Data, in_msg.Addr, cache_entry, tbe);
					}
				}
		//	}
		}
	}

	in_port(requestNetwork_in, RequestMsg, requestToCache, rank=0) {
		if (requestNetwork_in.isReady()) {

			peek(requestNetwork_in, RequestMsg, block_on="Addr") {

				Entry cache_entry :=getL1CacheEntry(in_msg.Addr);
				TBE tbe := TBEs[in_msg.Addr];

			// This assert should not cause problems, as a broadcast call should put all the machineIDs in the Destination
       	DPRINTF(RubySlicc, "address: %s, requestor: %s, Type: %s\n",
               in_msg.Addr, in_msg.Requestor, in_msg.Type);

				assert(in_msg.Destination.isElement(machineID));
				// Check if the request type is GETS or GETI
				if (in_msg.Type == CoherenceRequestType:GETS) {
					// Check if requestor is receiver
					if (in_msg.Requestor == machineID) {
						// Requestor is receiver. Seeing its own GETS
						trigger(Event:OWN_GETS, in_msg.Addr, cache_entry, tbe);
					}
					else {

						if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
							DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
							if (cache_entry.finalDestinationSet.isEmpty()) {
								cache_entry.finalDestinationSet.add(in_msg.Requestor);
								DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
							}
						}
						if (is_valid(cache_entry)) {
							DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, cache_entry.isAtomic);
						}
						if (is_valid(tbe)) {	
							DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, tbe.isAtomic);
						}
	
						if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
							DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s\n", in_msg.Addr);
							DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
						}
						else {
							trigger(Event:Other_GETS, in_msg.Addr, cache_entry, tbe);
						}
					}
				}

				else if (in_msg.Type == CoherenceRequestType:INV) {
					trigger(Event:INV, in_msg.Addr, cache_entry, tbe);
				}

				else if (in_msg.Type == CoherenceRequestType:GETI) {
					// Check if requestor is receiever
					if (in_msg.Requestor == machineID) {
						// Requestor is receiver. Seeing its own GETI
						trigger(Event:OWN_GETI, in_msg.Addr, cache_entry, tbe);
					}
					else {
						trigger(Event:Other_GETI, in_msg.Addr, cache_entry, tbe);
					}
				}
				else if (in_msg.Type == CoherenceRequestType:GETM) {
					// Check if requestor is receiver
					if (in_msg.Requestor == machineID) {
						// Requestor is receiver. Seeing its own GetM
						trigger(Event:OWN_GETM, in_msg.Addr, cache_entry, tbe);
					}
					else {
						DPRINTF(RubySlicc, "OTHER GETM requestor %s\n", in_msg.Requestor);
				
						if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
							DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
							if (cache_entry.finalDestinationSet.isEmpty()) {
								cache_entry.finalDestinationSet.add(in_msg.Requestor);
								DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
							}
						}
						if (is_valid(cache_entry)) {
							DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, cache_entry.isAtomic);
						}
						if (is_valid(tbe)) {	
							DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, tbe.isAtomic);
						}

						if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
							DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s", in_msg.Addr);
							DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
						}
						else {
							trigger(Event:Other_GETM, in_msg.Addr, cache_entry, tbe);
						}
					}
				}
				else if (in_msg.Type == CoherenceRequestType:GETMA) {
					// Check if requestor is receiver
					if (in_msg.Requestor == machineID) {
						// Requestor is receiver. Seeing its own GetM
						trigger(Event:OWN_GETM, in_msg.Addr, cache_entry, tbe);
					}
					else {
						DPRINTF(RubySlicc, "OTHER GETM requestor %s\n", in_msg.Requestor);
				
						if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
							DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
							if (cache_entry.finalDestinationSet.isEmpty()) {
								cache_entry.finalDestinationSet.add(in_msg.Requestor);
								DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
							}
						}
						if (is_valid(cache_entry)) {
							DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, cache_entry.isAtomic);
						}
						if (is_valid(tbe)) {	
							DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, tbe.isAtomic);
						}

						if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
							DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s", in_msg.Addr);
							DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
						}

					}
				}


				else if (in_msg.Type == CoherenceRequestType:GETX) {
					if (in_msg.Requestor == machineID) {
						trigger(Event:OWN_GETM, in_msg.Addr, cache_entry, tbe);
					}
					else {
						if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
							DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
							if (cache_entry.finalDestinationSet.isEmpty()) {
								cache_entry.finalDestinationSet.add(in_msg.Requestor);
								DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
							}
						}						
						if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
							DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s", in_msg.Addr);
							DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
						}
						else {
							trigger(Event:Other_GETM, in_msg.Addr, cache_entry, tbe);

						}			
					}
				}
				else if (in_msg.Type == CoherenceRequestType:PUTM) {
					if (in_msg.Requestor == machineID) {
						// Requestor is receiver. Seeing its own GETI
						trigger(Event:OWN_PUTM, in_msg.Addr, cache_entry, tbe);
					}
					else {
						trigger(Event:Other_PUTM, in_msg.Addr, cache_entry, tbe);
					}
				}
			}	
		}
	}
	
	in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...", rank=0) {
		if (mandatoryQueue_in.isReady()) { 
			if (is_blocked == false) {
				peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {
					Entry cache_entry := getL1CacheEntry(in_msg.LineAddress);

					DPRINTF(RubySlicc, "MSG ADDR: %s, MSG TYPE: %s is_invalid: %s, cache avail: %s\n",in_msg.LineAddress, in_msg.Type, is_invalid(cache_entry), Dcache.cacheAvail(in_msg.LineAddress));
					 //Checking what data is associated with Store
	
					if (in_msg.Type == RubyRequestType:IFETCH) {
						if (is_invalid(cache_entry) && Icache.cacheAvail(in_msg.LineAddress) == false) {
							trigger(Event:Replacement, Icache.cacheProbe(in_msg.LineAddress), 
											getL1CacheEntry(Icache.cacheProbe(in_msg.LineAddress)), 
											TBEs[Icache.cacheProbe(in_msg.LineAddress)]);
						}
						else {
							trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, 
										cache_entry, TBEs[in_msg.LineAddress]);
						}
					}
					else {
						if (is_invalid(cache_entry) && Dcache.cacheAvail(in_msg.LineAddress) == false) {
							trigger(Event:Replacement, Dcache.cacheProbe(in_msg.LineAddress), 
											getL1CacheEntry(Dcache.cacheProbe(in_msg.LineAddress)),
											TBEs[Dcache.cacheProbe(in_msg.LineAddress)]);
						}
						else {
							trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, cache_entry, TBEs[in_msg.LineAddress]);
						}
					}
				}
			}
			else {
				 //Allow RMW writes to go ahead
				peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {				
					Entry cache_entry := getL1CacheEntry(in_msg.LineAddress);
					DPRINTF(RubySlicc, "MSG ADDR: %s, MSG TYPE: %s is_invalid: %s, cache avail: %s\n",in_msg.LineAddress, in_msg.Type, is_invalid(cache_entry), Dcache.cacheAvail(in_msg.LineAddress));
					if (in_msg.Type == RubyRequestType:Locked_RMW_Write) {					
						trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, cache_entry, TBEs[in_msg.LineAddress]);
					}
				}
			}
		}
	} 
	
	
	/* For ruby random tester */
	/*
	in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...", rank=0) {
		if (mandatoryQueue_in.isReady()) { 
			if (is_blocked == false) {
				peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {
					Entry cache_entry := getL1CacheEntry(in_msg.LineAddress);

					DPRINTF(RubySlicc, "MSG ADDR: %s, MSG TYPE: %s is_invalid: %s, cache avail: %s\n",in_msg.LineAddress, in_msg.Type, is_invalid(cache_entry), Dcache.cacheAvail(in_msg.LineAddress));					 
					if (is_invalid(cache_entry) && Dcache.cacheAvail(in_msg.LineAddress) == false) {
						trigger(Event:Replacement, Dcache.cacheProbe(in_msg.LineAddress), 
										getL1CacheEntry(Dcache.cacheProbe(in_msg.LineAddress)),
										TBEs[Dcache.cacheProbe(in_msg.LineAddress)]);
					}
					else {
						trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, cache_entry, TBEs[in_msg.LineAddress]);
					}
					
				}
			}
			else {
				 //Allow RMW writes to go ahead
				peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {				
					Entry cache_entry := getL1CacheEntry(in_msg.LineAddress);
					DPRINTF(RubySlicc, "MSG ADDR: %s, MSG TYPE: %s is_invalid: %s, cache avail: %s\n",in_msg.LineAddress, in_msg.Type, is_invalid(cache_entry), Dcache.cacheAvail(in_msg.LineAddress));
					if (in_msg.Type == RubyRequestType:Locked_RMW_Write) {					
						trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, cache_entry, TBEs[in_msg.LineAddress]);
					}
				}
			}
		}
	}
	*/
	

  // ACTIONS
  action(as_issueGETSMem, "as", desc="Issue GETS") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
        out_msg.Addr := address;
        out_msg.Type := CoherenceRequestType:GETS;
        out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
        DPRINTF(RubySlicc, "ISSUE GETS address: %s, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
      }
    }
  }

 
  action(ai_issueGETINSTR, "ai", desc="Issue GETINSTR") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
        out_msg.Addr := address;
        out_msg.Type := CoherenceRequestType:GETI;
        out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
        DPRINTF(RubySlicc, "ISSUE GETINSTR address: %s, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;

      }
    }
  }

	action(bm_issueGETM, "bm", desc="Issue GETM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETM;
				out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
				//out_msg.Destination.broadcast(MachineType:MemCache);

				out_msg.Data := in_msg.Data64Bit;
				out_msg.DataOffset := in_msg.Offset;
				out_msg.DataSize := in_msg.Size;
        DPRINTF(RubySlicc, "ISSUE GETM address: %s, destination: %s \n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
			}
		}
	}

	action(bm_issueGETMA, "bma", desc="Issue GETM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETMA;
				out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));

				out_msg.Data := in_msg.Data64Bit;
				out_msg.DataOffset := in_msg.Offset;
				out_msg.DataSize := in_msg.Size;
        DPRINTF(RubySlicc, "ISSUE GETM address: %s, destination: %s \n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;

			}
		}
	}

	action(bam_issueGETM, "bama", desc="Issue GETM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETM;
				out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));


				out_msg.Data := in_msg.Data64Bit;
				out_msg.DataOffset := in_msg.Offset;
				out_msg.DataSize := in_msg.Size;
        DPRINTF(RubySlicc, "ISSUE GETM address: %s, destination: %s \n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;

			}
		}
	}

	action(bam_issueGETMATOMICST, "bmst", desc="Issue GETM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(atomicRequestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETMATOMICST;
				out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);				
				out_msg.Destination.remove(machineID);
        DPRINTF(RubySlicc, "ISSUE GETMATOMIC address: %s, destination: %s \n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control; 
			}
		}
	}

	action(bam_issueGETMATOMICEN, "bmen", desc="Issue GETM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(atomicRequestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETMATOMICEN;
				out_msg.Requestor := machineID;				
				out_msg.Destination.broadcast(MachineType:L1Cache);				
				out_msg.Destination.remove(machineID);
        DPRINTF(RubySlicc, "ISSUE GETMATOMIC address: %s, destination: %s \n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control; 
			}
		}
	}


	action(bpm_issuePUTM, "bpm", desc="Issue PUTM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:PUTM;
				out_msg.Requestor := machineID;
				// Need a broadcast actually
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
				DPRINTF(RubySlicc, "ISSUE PUTM address: %s, destination: %s\n",
                address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
				out_msg.DataBlk := cache_entry.DataBlk;
  		}
		}
	}

  action(uu_profileInstHit, "\uih", desc="Profile the demand hit") {
      ++Icache.demand_hits;
  }

	action(uu_profileInstMiss, "\uim", desc="Profile inst miss") {
			++Icache.demand_misses;
	}

	action(uu_profileWB, "\cwb", desc="Profile WB requests") {
			++Dcache.writeBacks; 
	}

  action(uu_profileDataMiss, "\udm", desc="Profile the demand miss") {
      ++Dcache.demand_misses;
  }

	action(uu_profileApproxLoadDataMiss, "\udma", desc="Profile the approx data miss") {
			++Dcache.approx_load_misses;
	}

	action(uu_profileApproxStoreDataMiss, "\usma", desc="Profile the approx data miss") {
			++Dcache.approx_store_misses;
	}

  action(uu_profileDataHit, "\udh", desc="Profile the demand hit") {
      ++Dcache.demand_hits;
  }

  action(uu_profileApproxLoadDataHit, "\uldh", desc="Profile the demand hit") {
      ++Dcache.approx_load_hits;
  }

  action(uu_profileApproxStoreDataHit, "\usdh", desc="Profile the demand hit") {
      ++Dcache.approx_store_hits;
  }


  action(uu_profileIstate_othergetM, "\ugm", desc="Profile the I state othergetm") {
      ++sequencer.Istate_othergetM;
  }

  action(uu_profilecoherence_inv, "\uci", desc="Profile the coherence invs") {
  	peek(requestNetwork_in, RequestMsg) {
  		assert(in_msg.Destination.isElement(machineID));
  		DPRINTF(RubySlicc, "coherence_inv: cache_entry: %s, address: %s\n", cache_entry,address);
  		if(is_valid(cache_entry)) {
  		  if(cache_entry.CacheState == State:IS_D || cache_entry.CacheState == State:IM_D || cache_entry.CacheState ==State:IM_DS) {
  		    ++sequencer.I_state_no_diff_check;
    	  }
   		 	else if(cache_entry.CacheState == State:SM_AD || cache_entry.CacheState==State:SM_D || cache_entry.CacheState==State:SM_DS) {
    	 		++sequencer.intermediate_state_diff_check;
    	 	} 
  		}
  	}
  	++sequencer.coherence_inv;
  }

	action(uu_profilecoherence_approx_inv, "\uca", desc="Profile the approx coherence invs") {
		++sequencer.approx_coherence_inv;
	}

	action(setBlocked, "\sb", desc="Block controller") {
		is_blocked := true;
	}

	action(setUnblocked, "\sub", desc="Unblock controller") {
		is_blocked := false;
	}


	action(bpm_issuePUTMD, "bpmD", desc="Issue BPM from request queue") {
		enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
			out_msg.Addr := address;
			out_msg.Type := CoherenceRequestType:PUTM;
			out_msg.Requestor := machineID;
			// Need a broadcast actually
			out_msg.Destination.broadcast(MachineType:L1Cache);
			out_msg.Destination.add(map_Address_to_Directory(address));
			//DPRINTF(RubySlicc, "ISSUE PUTM address: %s, destination: %s\n",address, out_msg.Destination);
       out_msg.MessageSize := MessageSizeType:Control;
			out_msg.DataBlk := cache_entry.DataBlk;
		}
	}

	action(bpm_issuePUTMR, "bpmr", desc="Issue BPM from request queue") {
  	peek(requestNetwork_in, RequestMsg) {
			enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:PUTM;
				out_msg.Requestor := machineID;
				// Need a broadcast actually
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
				//DPRINTF(RubySlicc, "ISSUE PUTM address: %s, destination: %s\n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
				out_msg.DataBlk := cache_entry.DataBlk;
			}
		}
	}

	action(cc_sendDataCacheToDir, "cc", desc="Send data from cache to directory") {
		enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
			assert(is_valid(cache_entry));
			out_msg.Addr := address;
			out_msg.Type := CoherenceResponseType:DATA_TO_WB;
			out_msg.Sender := machineID;
			out_msg.Destination.add(map_Address_to_Directory(address));
			DPRINTF(RubySlicc, "SEND DATA FROM CACHE TO DIR address: %s, destination: %s sender: %s, data %s\n",
                address, out_msg.Destination, out_msg.Sender, cache_entry.DataBlk);
      out_msg.DataBlk := cache_entry.DataBlk;
			out_msg.MessageSize := MessageSizeType:Control;
		}
	}

	action(cc_sendDataCacheToCache, "ccC", desc="Send data from cache to cache") {
		enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
			assert(is_valid(cache_entry));
			out_msg.Addr := address;
			out_msg.Type := CoherenceResponseType:DATA_TO_CACHE;
			out_msg.Sender := machineID;			
			//out_msg.Destination.add(cache_entry.finalDestination);
			out_msg.Destination.addNetDest(cache_entry.finalDestinationSet);
			//out_msg.Destination.broadcast(MachineType:L1Cache);
			DPRINTF(RubySlicc, "SEND DATA FROM CACHE TO CACHE address: %s, destination: %s sender: %s, data %s\n",
                address, out_msg.Destination, out_msg.Sender, cache_entry.DataBlk);
      out_msg.DataBlk := cache_entry.DataBlk;
			out_msg.MessageSize := MessageSizeType:Control;
			cache_entry.finalDestinationSet.clear();
		}
	}

	action(ct_sendDataTBEToDir, "ct", desc="Send data from TBE to directory") {
		enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
			assert(is_valid(tbe));
			out_msg.Addr := address;
			out_msg.Type := CoherenceResponseType:DATA_TO_WB;
			out_msg.Sender := machineID;			
			out_msg.Destination.add(map_Address_to_Directory(address));
			DPRINTF(RubySlicc, "SEND DATA FROM TBE TO DIR address: %s, destination: %s datablk %s\n",
                address, out_msg.Destination, tbe.DataBlk);
      out_msg.MessageSize := MessageSizeType:Control;
			out_msg.DataBlk := tbe.DataBlk;
     
		}
	}

	 
  action(forward_eviction_to_cpu, "\cc", desc="sends eviction information to the processor") {
    if (send_evictions) {
      DPRINTF(RubySlicc, "Sending invalidation for %s to the CPU\n", address);
      sequencer.evictionCallback(address);
    }
  }

  action(r_load_hit, "r",
         desc="new data from mem, notify sequencer the load completed.")
  {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "LOAD HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    sequencer.readCallback(address, cache_entry.DataBlk, false);
  }

  action(rx_load_hit, "rx",
         desc="data already present, notify sequencer the load completed.")
  {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "CACHED LOAD HIR Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    sequencer.readCallback(address, cache_entry.DataBlk, true);
  }

  action(s_store_hit, "s",
         desc="If not prefetch, notify sequencer that store completed.")
  {
    assert(is_valid(cache_entry));   
    sequencer.writeCallback(address, cache_entry.DataBlk);
    DPRINTF(RubySlicc, "STORE HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    cache_entry.Dirty := true;
  }

  action(sx_store_hit, "sx",
         desc="If not prefetch, notify sequencer that store completed.")
  {
    assert(is_valid(cache_entry));
    sequencer.writeCallback(address, cache_entry.DataBlk, true);
    DPRINTF(RubySlicc, "CACHED STORE HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    cache_entry.Dirty := true;		
  }


  action(sx_atomic_store_hit_set, "sax",
         desc="If not prefetch, notify sequencer that store completed.")
  {
    assert(is_valid(cache_entry));
    sequencer.writeCallback(address, cache_entry.DataBlk, true);
    DPRINTF(RubySlicc, "CACHED STORE HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    cache_entry.Dirty := true;
		cache_entry.isAtomic := true;
  }
 
 action(sx_atomic_store_hit_unset, "saxu",
         desc="If not prefetch, notify sequencer that store completed.")
  {
    assert(is_valid(cache_entry));
    sequencer.writeCallback(address, cache_entry.DataBlk, true);
    DPRINTF(RubySlicc, "CACHED STORE HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    cache_entry.Dirty := true;
		cache_entry.isAtomic := false;
  }

  action(i_allocateAtomicTBE, "iato", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(TBEs);
    assert(is_valid(cache_entry));
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
		DPRINTF(RubySlicc, "Putting TBE DataBlk for replacement:%s\n", tbe.DataBlk);
		tbe.isAtomic := true;
  }


  action(i_allocateTBE, "i", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(TBEs);
    assert(is_valid(cache_entry));
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
		DPRINTF(RubySlicc, "Putting TBE DataBlk for replacement:%s\n", tbe.DataBlk);
		tbe.isAtomic := false;
  }

  action(k_popMandatoryQueue, "k", desc="Pop mandatory queue.") {
    mandatoryQueue_in.dequeue();
  }

  action(l_popRequestQueue, "l",
    desc="Pop incoming request queue and profile the delay within this virtual network") {
    profileMsgDelay(2, requestNetwork_in.dequeue());
  }

 action(l_popAtomicRequestQueue, "lat",
    desc="Pop incoming request queue and profile the delay within this virtual network") {
    profileMsgDelay(2, atomicRequestNetwork_in.dequeue());
  }

  action(o_popIncomingResponseQueue, "o",
    desc="Pop Incoming Response queue and profile the delay within this virtual network") {
    profileMsgDelay(1, responseNetwork_in.dequeue());
  }

  action(s_deallocateTBE, "sd", desc="Deallocate TBE") {
    TBEs.deallocate(address);
    unset_tbe();
  }

  action(u_writeDataToL1Cache, "u", desc="Write data to cache") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
			DPRINTF(RubySlicc, "DataBlk write: %s\n", in_msg.DataBlk);
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(q_updateAckCount, "q", desc="Update ack count") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(tbe));
      tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

	action(unset_cache_entry, "uce", desc="unset cache entry") {
		unset_cache_entry();
	}

  action(ff_deallocateL1CacheBlock, "\f", desc="Deallocate L1 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    if (Dcache.isTagPresent(address)) {
      Dcache.deallocate(address);
    }
		else {
      Icache.deallocate(address);
    }
    unset_cache_entry();
  }

  action(oo_allocateAtomicL1DCacheBlock, "\o", desc="Set L1 D-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(Dcache.allocate(address, new Entry));
    }		

		DPRINTF(RubySlicc, "Atomic allocate\n");
		DPRINTF(RubySlicc, "IS VALID: %s", is_valid(cache_entry));
		cache_entry.isAtomic := true;
  }

  action(oo_allocateL1DCacheBlock, "\oatom", desc="Set L1 D-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(Dcache.allocate(address, new Entry));
    }

  }

  action(pp_allocateL1ICacheBlock, "\p", desc="Set L1 I-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(Icache.allocate(address, new Entry));
    }
  }	

  action(z_stallAndWaitMandatoryQueue, "\z", desc="recycle L1 request queue") {
		DPRINTF(RubySlicc, "Stalling address: %s\n", address);
    stall_and_wait(mandatoryQueue_in, address);
  }

	action(z_stallAndWaitRequestQueue, "\zresp", desc="recycle L1 response queue") {
		DPRINTF(RubySlicc, "Stalling response queue: %s\n", address);
		stall_and_wait(requestNetwork_in, address);		
	}

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
		if (isAtomic(address)) {
			DPRINTF(RubySlicc, "Cannot wake up dependents. Atomic access blocked: %s\n", address);
		}
		else {
    	wakeUpBuffers(address);
		}
  }

	action(ka_wakeUpAllDependents, "ka", desc="wake-up all dependents") {		
		if (isAtomic(address)) {
			DPRINTF(RubySlicc, "Cannot wake up dependents. Atomic access blocked: %s\n", address);
		}
		else {
    	wakeUpAllBuffers();
		}
	}


  //*****************************************************
  // TRANSITIONS
  //*****************************************************

	transition(I, INV) {
		// Do nothing
		l_popRequestQueue;
	}

	transition({IS_AD,IS_D,IS_A,IS_DI,IM_AD,IM_D,IM_A,IM_DI,IM_DS,IM_DSI,SM_AD,SM_D,SM_A,SM_DI,SM_DS,SM_DSI,MI_A,II_A}, INV) {
		z_stallAndWaitRequestQueue;
	}

	// Transition from M to I
	transition(M, {Other_GETS, Other_GETI}, MS_A) {
		bpm_issuePUTMR;

		l_popRequestQueue;
	}

	transition(M, {Other_GETM}, MI_A) {
		uu_profilecoherence_inv;
		bpm_issuePUTMR;
		l_popRequestQueue;
	}
	

	// Transition from I to IS_AD
	transition(I, Load, IS_AD) {	
		oo_allocateL1DCacheBlock;	
		i_allocateTBE;
		uu_profileDataMiss;
		as_issueGETSMem;
		k_popMandatoryQueue;
	}

	// Transition from I to IS_AD
	transition(I, Load_Approx, IS_AD) {	
		oo_allocateL1DCacheBlock;	
		i_allocateTBE;
		uu_profileApproxLoadDataMiss;
		as_issueGETSMem;
		k_popMandatoryQueue;
	}

	transition(I, Ifetch, IS_AD) {		
		pp_allocateL1ICacheBlock;	
		i_allocateTBE;
		uu_profileInstMiss;
		ai_issueGETINSTR;
		k_popMandatoryQueue;
	}

	// Transition from I to IM_AD
	transition(I, Store, IM_AD) {
		oo_allocateL1DCacheBlock;
		i_allocateTBE;
		uu_profileDataMiss;
		bm_issueGETM;
		k_popMandatoryQueue;
	}

	// Transition from I to IM_AD
	transition(I, Store_Approx, IM_AD) {
		oo_allocateL1DCacheBlock;
		i_allocateTBE;
		uu_profileApproxStoreDataMiss;
		bm_issueGETMA;
		k_popMandatoryQueue;
	}

	// Transition from IS_AD to IS_D
	transition(IS_AD, {OWN_GETS, OWN_GETI}, IS_D) {
		l_popRequestQueue;
	}

	// Transition from IM_AD to IM_D
	transition(IM_AD, OWN_GETM, IM_D) {
		l_popRequestQueue;
	}

	// Transition from IS_D to S
	transition(IS_D, Data, S) {
		u_writeDataToL1Cache;
		r_load_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}

	// Transition from IM_D to M
	transition(IM_D, Data, M) {
		u_writeDataToL1Cache;
		s_store_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}


	// Transition from IS_D to IS_DI
	transition(IS_D, Other_GETM, IS_DI) {
	  uu_profilecoherence_inv;
		l_popRequestQueue;
	}


	// Transition from IS_DI to I
	transition(IS_DI, Data, I) {
		u_writeDataToL1Cache;
		r_load_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
    ff_deallocateL1CacheBlock;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}

	// Transition from IS_AD to IS_A
	transition(IS_AD, Data, IS_A) {
		u_writeDataToL1Cache;
		s_deallocateTBE;
		o_popIncomingResponseQueue;		
	}

	// Transition from IS_A to S
	transition(IS_A, {OWN_GETS,OWN_GETI}, S) {
		r_load_hit;
		l_popRequestQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}

	//Transition from IM_AD to IM_A
	transition(IM_AD, Data, IM_A) {
		u_writeDataToL1Cache;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
	}

	// Transition from IM_A to M
	transition(IM_A, OWN_GETM, M) {
		s_store_hit;
		l_popRequestQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}

	// Transition from IM_D to IM_DS
	transition(IM_D, {Other_GETS,Other_GETI}, IM_DS) {
		l_popRequestQueue;
	}

	transition(IM_DS, Data, MS_A) {
		u_writeDataToL1Cache;
		s_deallocateTBE;
		s_store_hit;
		bpm_issuePUTMD;

		o_popIncomingResponseQueue;
	}


	// Transition from IM_D to IM_DI
	transition(IM_D, Other_GETM, IM_DI) {
	  uu_profilecoherence_inv;
		l_popRequestQueue;
	}


	transition(IM_DI, Data, MI_A) {
		u_writeDataToL1Cache;
		s_deallocateTBE;
		s_store_hit;
		bpm_issuePUTMD;

		o_popIncomingResponseQueue;
	}


	// Transition from IM_DS to IM_DSI
	transition(IM_DS, Other_GETM, IM_DSI) {
	  uu_profilecoherence_inv;
		l_popRequestQueue;
	}


	transition({IM_DS,SM_DS}, Other_GETS) {
		l_popRequestQueue;
	}

	// Transition from IM_DSI to I
	transition(IM_DSI, Data, I) {
		u_writeDataToL1Cache;
		s_deallocateTBE;
		s_store_hit;
		cc_sendDataCacheToDir;		
		uu_profileWB;
		ff_deallocateL1CacheBlock;
		o_popIncomingResponseQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}


	// Transition from S to I
	transition(S, Other_GETM, I) {
	  uu_profilecoherence_inv;
		ff_deallocateL1CacheBlock;
		l_popRequestQueue;
	}
	

	// Transition from S to SM_AD
	transition(S, Store, SM_AD) {
		bm_issueGETM;	
		i_allocateTBE;
		uu_profileDataMiss;
		k_popMandatoryQueue;
	}

	// Transition from S to SM_AD
	transition(S, Store_Approx, SM_AD) {
		bm_issueGETMA;	
		i_allocateTBE;
		uu_profileApproxStoreDataMiss;
		k_popMandatoryQueue;
	}

	// Transition from SM_AD to SM_D
	transition(SM_AD, OWN_GETM, SM_D) {
		l_popRequestQueue;
	}

	// Transition from SM_D to M
	transition(SM_D, Data, M) {	
		u_writeDataToL1Cache;
		sx_store_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}
	

	// Transition from SM_AD to IM_AD
	transition(SM_AD, Other_GETM, IM_AD) {
	  uu_profilecoherence_inv;
		l_popRequestQueue;
	}


	transition(SM_AD, Data, SM_A) {
		u_writeDataToL1Cache;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
	}
	
	transition(SM_A, OWN_GETM, M) {
		sx_store_hit;
		l_popRequestQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}
	

	transition(SM_A, Other_GETM, IM_A) {
	  uu_profilecoherence_inv;
		l_popRequestQueue;
	}
	

	// Transition from SM_D to SM_DS
	transition(SM_D, {Other_GETS,Other_GETI}, SM_DS) {
		l_popRequestQueue;
	}


	// Transition from SM_D to SM_DI
	transition(SM_D, Other_GETM, SM_DI) {
	  uu_profilecoherence_inv;
		l_popRequestQueue;
	}
	

	// Transition from SM_DS to S
	transition(SM_DS, Data, S) {
		u_writeDataToL1Cache;
		sx_store_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;	
		cc_sendDataCacheToDir;
		uu_profileWB;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}


	// Transition from SM_DS to S
	transition(SM_DSI, Data, I) {
		u_writeDataToL1Cache;
		sx_store_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;	
		cc_sendDataCacheToDir;
		uu_profileWB;
		ff_deallocateL1CacheBlock;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}
	
	// Transition from SM_DI to I
	transition(SM_DI, Data, I) {
		u_writeDataToL1Cache;
		sx_store_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		cc_sendDataCacheToDir;
		uu_profileWB;
		ff_deallocateL1CacheBlock;
 		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}


	// Transition from SM_DS to SM_DSI
	transition(SM_DS, Other_GETM, SM_DSI) {
	  uu_profilecoherence_inv;
		l_popRequestQueue;
	}
	
	// Transition from M to MI_A
	transition(M, Replacement, MI_A) {
		//i_allocateTBE;
		// The funda to not put the i_allocateTBE is that when you do this and you geta store hit, the cache contents is updated but not the TBE. So, allocateTBE only when you see the putM.
		bpm_issuePUTM;	

		z_stallAndWaitMandatoryQueue;				
		// Cannot pop mandatory queue for Replacement, as it is a side event caused by a Load or store
	}

	transition({MI_A,MS_A}, Replacement) {
		z_stallAndWaitMandatoryQueue;
	}

	transition(S, {INV,Replacement}, I) {
		ff_deallocateL1CacheBlock;		
	}

	// Transition from MI_A to I
	transition(MI_A, OWN_PUTM, I) {
		i_allocateTBE;
		ct_sendDataTBEToDir;
		uu_profileWB;
		ff_deallocateL1CacheBlock;
		l_popRequestQueue;
		ka_wakeUpAllDependents;
		// ANI: Should I deallocate it here or after I recieve an ack from the directory?
		s_deallocateTBE;
	}

	transition(MS_A, {Other_GETS, Other_GETI}) {
		l_popRequestQueue;
	}

	transition(MS_A, {Other_GETM}, MI_A) {
		l_popRequestQueue;
	}

	transition(MS_A, {Ifetch, Load, Store, RMW_Read}) {
		z_stallAndWaitMandatoryQueue;
	}

	transition(MS_A, OWN_PUTM, S) {
		i_allocateTBE;
		ct_sendDataTBEToDir;		
		uu_profileWB;
		l_popRequestQueue;
		ka_wakeUpAllDependents;
		s_deallocateTBE;
	}

	transition(M, INV, I) {
		i_allocateTBE;
		ct_sendDataTBEToDir;
		uu_profileWB;
		ff_deallocateL1CacheBlock;
		l_popRequestQueue;
		ka_wakeUpAllDependents;
		// ANI: Should I deallocate it here or after I recieve an ack from the directory?
		s_deallocateTBE;	
	}	

	transition(MI_A, {Other_GETS, Other_GETI, Other_GETM}) {
		l_popRequestQueue;
	}

	transition(I, {Other_GETM}) {
	  uu_profileIstate_othergetM;
		l_popRequestQueue;
	}

	transition(I, {Other_GETS, Other_GETI}) {
		l_popRequestQueue;
	}

	transition({S,IS_D}, {Other_GETI, Other_GETS}) {
		l_popRequestQueue;
	}

	// Transition from II_A to I
	transition(II_A, OWN_PUTM, I) {
		// Deallocate L1 cache block when moving to I
		ff_deallocateL1CacheBlock;
		l_popRequestQueue; 
		ka_wakeUpAllDependents;
	}

	// Stall transitions
	transition({IS_AD, IS_D, IS_A, IS_DI, IM_AD, IM_D, IM_A, IM_DI, IM_DS, IM_DSI, II_A}, {Load, Load_Approx,Ifetch, Store, Store_Approx,Replacement, RMW_Read, RMW_Write}) {
		z_stallAndWaitMandatoryQueue;
	}

	transition({IS_AD, IM_AD, IM_A, II_A}, {Other_GETS, Other_GETI, Other_PUTM}) {
		l_popRequestQueue;
	}

	transition({IS_AD, IM_AD, IM_A, II_A}, {Other_GETM}) {
	  uu_profileIstate_othergetM;
		l_popRequestQueue;
	}


	transition({IM_DI, IS_A, IM_DSI, SM_DI, SM_DSI, IS_DI}, {Other_GETS, Other_GETI}) {
		l_popRequestQueue;
	}

	transition({IM_DI, IS_A, IM_DSI, SM_DI, SM_DSI, IS_DI}, {Other_GETM}) {
	  uu_profileIstate_othergetM;
		l_popRequestQueue;
	}

	transition({S, SM_DS, SM_AD, SM_D, SM_A, SM_DI, SM_DSI, M, MI_A}, Load) {
		rx_load_hit;
		uu_profileDataHit;
		k_popMandatoryQueue;	
	}

	transition({S, SM_DS, SM_AD, SM_D, SM_A, SM_DI, SM_DSI, M, MI_A}, Load_Approx) {
		rx_load_hit;
		uu_profileApproxLoadDataHit;
		k_popMandatoryQueue;	
	}

	transition({S, SM_DS, SM_AD, SM_D, SM_A, SM_DI, SM_DSI, M, MI_A}, {Ifetch}) {
		rx_load_hit;
		uu_profileInstHit;
		k_popMandatoryQueue;	
	}

	transition({SM_AD, SM_D, SM_A, SM_DI, SM_DSI, SM_DS}, {Replacement, Store, Store_Approx}) {
		z_stallAndWaitMandatoryQueue;
	}

	transition({M, MI_A}, Store) {
		sx_store_hit;
		uu_profileDataHit;
		k_popMandatoryQueue;
	}

	transition({MI_A}, RMW_Read) {
		z_stallAndWaitMandatoryQueue;
	}

	transition({M, MI_A}, Store_Approx) {
		sx_store_hit;		
		uu_profileApproxStoreDataHit;
		k_popMandatoryQueue;
	}

	transition({I,S}, Other_PUTM) {
		l_popRequestQueue;
	}

	transition({I,II_A, S}, Data) {	
		o_popIncomingResponseQueue;	
	}

	transition({SM_AD,SM_A}, Other_GETS) {
		l_popRequestQueue;
	}

	// Rationale is that when SM_AD state sees other getS, it registers requestor in final destination net
	// When it receives the data, it will send it to the requestor who would have received it from main memory 
	// So just ignore the data and move on
	transition(M, Data) {
		o_popIncomingResponseQueue;
	}

////////////////////////////////////

  // RMW.
	// On RMW-Read, broadcast atomic st signal to all
	transition(M, RMW_Read) {
		bam_issueGETMATOMICST;
		sx_atomic_store_hit_set;
		k_popMandatoryQueue;
	}

	transition(I, RMW_Read, IM_AD) {		
		oo_allocateAtomicL1DCacheBlock;	
		i_allocateAtomicTBE;
		bam_issueGETM;
		bam_issueGETMATOMICST;
		k_popMandatoryQueue;	
	}

	// Cannot have I to RMW_Write
	// Cannot have S to RMW_Write

	transition(M, RMW_Write) {
		bam_issueGETMATOMICEN;
		sx_atomic_store_hit_unset;
		k_popMandatoryQueue;
	}

	
	transition(S, RMW_Read, SM_AD) {		
		oo_allocateAtomicL1DCacheBlock;
		i_allocateAtomicTBE;
		bam_issueGETM;
		bam_issueGETMATOMICST;
		k_popMandatoryQueue;
	}


//////////////////////////////////////

	transition({I, M, S, IS_AD, IS_D, IS_DI, IS_A, IM_AD, IM_D, IM_A, IM_DS, IM_DI, IM_DSI, SM, IS_I, IM_S, IM_SI, SM_I, SM_S, SM_SI, SM_AD, SM_A, SM_D, SM_DS, SM_DSI, SM_DI, M_I, MI_A, II_A, I_I, MS_A}, ATOMICST) {
		setBlocked;	
		l_popAtomicRequestQueue;
	}

	transition({I, M, S, IS_AD, IS_D, IS_DI, IS_A, IM_AD, IM_D, IM_A, IM_DS, IM_DI, IM_DSI, SM, IS_I, IM_S, IM_SI, SM_I, SM_S, SM_SI, SM_AD, SM_A, SM_D, SM_DS, SM_DSI, SM_DI, M_I, MI_A, II_A, I_I, MS_A}, ATOMICEN) {	
		setUnblocked;
		l_popAtomicRequestQueue;
	}

	// This case cannot happen according to the book
	// A case where this can happen is stored in error-transition-1.log
	transition({IM_D,IM_DI, IM_DS, IM_DSI, M, IS_D, IS_DI ,MI_A, MS_A}, Other_PUTM) {
		l_popRequestQueue;
	}
}
