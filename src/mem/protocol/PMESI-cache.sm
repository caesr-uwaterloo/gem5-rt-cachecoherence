/*
 * Copyright (c) 1999-2013 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OtherWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*/

machine(L1Cache, "MSI Snooping L1 Cache CMP")
 : Sequencer * sequencer;
   CacheMemory * Icache;
   CacheMemory * Dcache;
   Cycles l1_request_latency := 5;
   Cycles l1_response_latency := 2;
   bool send_evictions;
	 int is_blocked;	 

	 Cycles max_wcet_latency := 0;
	 Cycles max_arbit_latency := 0;
	 Cycles max_interC_latency := 0;
	 Cycles max_intraC_latency := 0;
	
   // Message Queues

	 // To the network
	 // requestFromCache: Cache controller will receive messages from the cache and propogate to the bus
	 MessageBuffer * requestFromCache, network="To", virtual_network="2", ordered="false", vnet_type="request";

	 MessageBuffer * requestFromCacheWB, network="To", virtual_network="6", ordered="false", vnet_type="requestWB";
	 
	 // responseFromCache: Cache controller will respond to cache activity from other cores
	 MessageBuffer * responseFromCache, network="To", virtual_network="4", ordered="false", vnet_type="response";

	 // Atomic ack vnet
	 MessageBuffer * atomicRequestFromCache, network="To", virtual_network="5", ordered="false", vnet_type="atomicRequest";

	 // From the network
	 // responseToCache: Cache controller will recieve replies based on the requests it issued on responeFromCache network
	 MessageBuffer * responseToCache, network="From", virtual_network="4", ordered="false", vnet_type="response";
	 // requestToCache: Cache controller will recieve replies based on other cache controller activities
	 MessageBuffer * requestToCache, network="From", virtual_network="2", ordered="false", vnet_type="request";

	 MessageBuffer * requestToCacheWB, network="From", virtual_network="6", ordered="false", vnet_type="requestWB"; 

	// Atomic ack vnet
	MessageBuffer * atomicRequestToCache, network="From", virtual_network="5", ordered="false", vnet_type="atomicRequest";
{
  // STATES
  state_declaration(State, desc="Cache states", default="L1Cache_State_I") {
    // Base states
    NP, AccessPermission:Invalid, desc="Not present in either cache";
    I, AccessPermission:Invalid, desc="a L1 cache entry Idle";
    S, AccessPermission:Read_Only, desc="a L1 cache entry Shared";
    M, AccessPermission:Read_Write, desc="a L1 cache entry Modified", format="!b";
		E, AccessPermission:Read_Write, desc="a L1 exclusive cache entry", format="!b";

    // Transient States
    IS_AD, AccessPermission:Busy, desc="L1 idle, issued a load, have not seen response yet";
    fIS_AD, AccessPermission:Busy, desc="L1 idle, issued a load, have not seen response yet";
    IS_D, AccessPermission:Busy, desc="L1 idle, issued a load, saw own GETS, not seen data response yet";
    fIS_D, AccessPermission:Busy, desc="L1 idle, issued a load, saw own GETS, not seen data response yet";
    IS_DI, AccessPermission:Busy, desc="L1 idle, issued a load, saw own GETS, then saw remote GETM not seen data response yet";
		IM_AD, AccessPermission:Busy, desc="L1 idle, issued GETX, have not seen response yet";
		IM_D, AccessPermission:Busy, desc="L1 idle, issued GETX, saw own GETX, not seen data response yet";
		IM_DS, AccessPermission:Busy, desc="lisnfd";
		IM_DI, AccessPermission:Busy, desc="lisnfd";
		SM_AD, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
		SM_D, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
		SM_DS, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
		SM_DI, AccessPermission:Read_Only, desc="L1 read-only, issued GetX, saw another remote core do GetS, followed by a GetX. Send data to the GetX and GetS cores.";
    MI_A, AccessPermission:Read_Write, desc="L1 replacing, waiting for ACK. Serive remote load and store misses";
    EI_A, AccessPermission:Read_Only, desc="L1 replacing, waiting for ACK. Serive remote load and store misses";
    ES_A, AccessPermission:Read_Only, desc="L1 replacing, waiting for ACK. Serive remote load and store misses";
    MS_A, AccessPermission:Read_Write, desc="L1 replacing, waiting for ACK. Serive remote load and store misses";

		M_L, AccessPermission:Busy, desc="M waiting for ATOMICEN";
		SM_WL, AccessPermission:Busy, desc="SW_WL";
		IM_WL, AccessPermission:Busy, desc="SW_WL";
		IM_ADL, AccessPermission:Busy, desc="IM_ADL";
		IM_DL, AccessPermission:Busy, desc="IM_DL";
		IM_AL, AccessPermission:Busy, desc="IM_DL";
  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
		
		// Processor generated events
		Load, desc="Load request from processor";
		Store, desc="Store request from processor";
		Ifetch, desc="Instruction fetch request from processor";
		RMW_Read, desc="Read-modify-write read from processor";
		RMW_Write, desc="Read-modify-write write from processor";
		Replacement, desc="Replacement from processor";
		Load_Approx, desc="Load approx request from processor";
		Store_Approx, desc="Store approx request from processor";

		// Requests observed on the bus
		Other_GETS, desc="Remote processor doing load";
		Other_GETM, desc="Remote processor doing store";
		Other_GETM_Approx, desc="Remote processor doing store";
		Other_GETI, desc="Remote processor doing instruction fetch";
		Other_PUTM, desc="Remote processor doing a PUTM";
		OWN_GETS, desc="Processor sees its own GETS coherence message ordered on the network";
		OWN_GETM, desc="Processor sees its own GETM coherence message ordered on the network";
		OWN_GETI, desc="Procssor sees its own GETI coherence message ordered on the network";
		OWN_PUTM, desc="Processor sees its own PUTM";
		INV, desc="Processor sees INV signal from Mem";

		// Atomic requests
		ATOMICST, desc="Atomic request";
		ATOMICEN, desc="Atomic request";

		// Data observed on the bus
		Data, desc="Data sent by shared mem. No cache-to-cache transfer";
		EData, desc="Exclusive data sent by shared mem";

	  // Acks
		Ack, desc="Ack for processor";
  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry" ) {
    State CacheState,        desc="cache state";
    DataBlock DataBlk,       desc="data for the block";
    bool Dirty, default="false",   desc="data is dirty";
		// No unsigned int
		MachineID finalDestination, desc="where this cache block should go in cache-to-cache transfer";
		NetDest finalDestinationSet, desc="where this cache block should go in cache-to-cache transfer";
		// Is this an atomic request?
		bool isAtomic, default="false", desc="data is atomic";
  }

  // TBE fields
  structure(TBE, desc="...") {
    Address Addr,              desc="Physical address for this TBE";
    State TBEState,        desc="Transient state";
    DataBlock DataBlk,                desc="Buffer for the data block";
    bool Dirty, default="false",   desc="data is dirty";
    int pendingAcks, default="0", desc="number of pending acks";
		bool isAtomic, default="false", desc="data is atomic";

		// For wcet accounting
		Cycles startTime, default="Cycles(0)", desc="start time of request";
		Cycles endTime, default="Cycles(0)", desc="end time of request";
		Cycles arbitLatency, default="Cycles(0)", desc="arbit latency of request";
		Cycles interCoreCohLatency, default="Cycles(0)", desc="inter-core coherence latency of request";
		Cycles intraCoreCohLatency, default="Cycles(0)", desc="intra-core coherence latency of request";
		Cycles firstScheduledTime, default="Cycles(0)", desc="first scheduled time";
		int CoreID;
		bool zeroArb, default="false", desc="zero arbitration";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }

  TBETable TBEs, template="<L1Cache_TBE>", constructor="m_number_of_TBEs";

  MessageBuffer mandatoryQueue, ordered="false";
	
  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Address a);
	void wakeUpAllBuffers();
  void profileMsgDelay(int virtualNetworkType, Cycles c);

  // inclusive cache returns L1 entries only
  Entry getL1CacheEntry(Address addr), return_by_pointer="yes" {
		Entry Dcache_entry := static_cast(Entry, "pointer", Dcache[addr]);
    if(is_valid(Dcache_entry)) {
      return Dcache_entry;
    }

    Entry Icache_entry := static_cast(Entry, "pointer", Icache[addr]);
    return Icache_entry;
  }

	bool isAtomic(Address addr) {
	  //TBE tbe := TBEs[addr];
		//if (is_valid(tbe)) {
		//	return tbe.isAtomic;
		//}

    Entry cache_entry := getL1CacheEntry(addr);
		if (is_valid(cache_entry)) {
			return cache_entry.isAtomic;
		}
		return false;
	}

  Entry getDCacheEntry(Address addr), return_by_pointer="yes" {
    Entry Dcache_entry := static_cast(Entry, "pointer", Dcache[addr]);
    return Dcache_entry;
  }

  Entry getICacheEntry(Address addr), return_by_pointer="yes" {
    Entry Icache_entry := static_cast(Entry, "pointer", Icache[addr]);
    return Icache_entry;
  }

  State getState(TBE tbe, Entry cache_entry, Address addr) {

    assert((Dcache.isTagPresent(addr) && Icache.isTagPresent(addr)) == false);

    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:I;
  }

	bool is_dataSendState(TBE tbe, Address addr, Entry cache_entry) {	
		State returnState := getState(tbe, cache_entry, addr);
		if (returnState == State:M || returnState == State:IM_D || returnState == State:SM_D || returnState == State:MI_A) {
			return true;
		}
		return false;
	}

	bool is_dataWaitingState(TBE tbe, Address addr, Entry cache_entry) {
		State returnState := getState(tbe, cache_entry, addr);
		// This function is for atomic operations
		if (returnState == State:IM_D || returnState == State:SM_D || returnState == State:M) {
			return true;
		}
		return false;
	}

	bool is_atomicInvariant(TBE tbe, Address addr, Entry cache_entry) {
		State returnState := getState(tbe, cache_entry, addr);
		if (returnState == State:M || returnState == State:S || returnState == State:I) {
			return false;
		}
		return true;
	}

  void setState(TBE tbe, Entry cache_entry, Address addr, State state) {

    assert((Dcache.isTagPresent(addr) && Icache.isTagPresent(addr)) == false);

    // MUST CHANGE
    if(is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }

  }

  AccessPermission getAccessPermission(Address addr) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s addr: %s\n", L1Cache_State_to_permission(tbe.TBEState), addr);
      return L1Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getL1CacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s addr: %s\n", L1Cache_State_to_permission(cache_entry.CacheState), addr);
      return L1Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s addr: %s\n", AccessPermission:NotPresent, addr);
    return AccessPermission:NotPresent;
  }

  void functionalRead(Address addr, Packet *pkt) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      testAndRead(addr, tbe.DataBlk, pkt);
    } else {
      testAndRead(addr, getL1CacheEntry(addr).DataBlk, pkt);
    }
  }

  int functionalWrite(Address addr, Packet *pkt) {
    int num_functional_writes := 0;

    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      num_functional_writes := num_functional_writes +
        testAndWrite(addr, tbe.DataBlk, pkt);
      return num_functional_writes;
    }

    num_functional_writes := num_functional_writes +
        testAndWrite(addr, getL1CacheEntry(addr).DataBlk, pkt);
    return num_functional_writes;
  }

  void setAccessPermission(Entry cache_entry, Address addr, State state) {

    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L1Cache_State_to_permission(state));
    }

  }

  Event mandatory_request_type_to_event(RubyRequestType type) {
    if (type == RubyRequestType:LD || type == RubyRequestType:LD_SHARED) {
      ++sequencer.loads;
      return Event:Load;
    } 
		else if (type == RubyRequestType:LD_APPROX) {
			++sequencer.loads;
			++sequencer.approx_loads;
			return Event:Load_Approx;
		}
		else if (type == RubyRequestType:IFETCH) {
      ++sequencer.ifetch;
      return Event:Ifetch;
      //return Event:Load;
    } else if ((type == RubyRequestType:ST) || (type == RubyRequestType:ATOMIC) || (type == RubyRequestType:ST_SHARED)) {
      ++sequencer.stores;
      return Event:Store;
    }
		else if (type == RubyRequestType:ST_APPROX) {
			++sequencer.stores;
			++sequencer.approx_stores;
			// Record difference in data
			return Event:Store_Approx;
		}
		else if (type == RubyRequestType:Locked_RMW_Read) {
			// RMW_Write should finally transition to an M_ATOMIC state that RMW_Write will pick up from
			return Event:RMW_Read;
			++sequencer.stores;
		}
		else if (type == RubyRequestType:Locked_RMW_Write) {
			// RMW_Write should finally transition to an M state
			return Event:RMW_Write;
			++sequencer.stores;
		}
		
		else {
      error("Invalid RubyRequestType");
    }
  }

  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }

	// ** OUT_PORTS **
  out_port(requestNetwork_out, RequestMsg, requestFromCache);
  out_port(responseNetwork_out, ResponseMsg, responseFromCache);
	out_port(atomicRequestNetwork_out, ResponseMsg, atomicRequestFromCache);
	out_port(requestNetworkWB_out, RequestMsg, requestFromCacheWB);

	in_port(atomicRequestNetwork_in, RequestMsg, atomicRequestToCache, rank=1) {
		if (atomicRequestNetwork_in.isReady()) {
			peek(atomicRequestNetwork_in, RequestMsg, block_on="Addr") {
				Entry cache_entry := getL1CacheEntry(in_msg.Addr);
				TBE tbe := TBEs[in_msg.Addr];

				if (in_msg.Type == CoherenceRequestType:GETMATOMICEN) {
					DPRINTF(RubySlicc, "ATOMIC EN request issued on address: %s\n", in_msg.Addr);
					trigger(Event:ATOMICEN, in_msg.Addr, cache_entry, tbe);
					/*
					if (in_msg.Requestor != machineID) {
						DPRINTF(RubySlicc, "ATOMIC EN request issued on address: %s\n", in_msg.Addr);
						trigger(Event:ATOMICEN, in_msg.Addr, cache_entry, tbe);
					}
					else {
						atomicRequestNetwork_in.dequeue();
					}
					*/
				}
				else if (in_msg.Type == CoherenceRequestType:GETMATOMICST) {
					if (in_msg.Requestor != machineID) {
						DPRINTF(RubySlicc, "ATOMIC request issued on address: %s\n", in_msg.Addr);
						trigger(Event:ATOMICST, in_msg.Addr, cache_entry, tbe);
					}
					else {
						atomicRequestNetwork_in.dequeue();
					}
				}
			}
		}
	}

	// Response to cache
	in_port(responseNetwork_in, ResponseMsg, responseToCache, rank=1) {
		if (responseNetwork_in.isReady()) {
			//if (is_blocked == false) {
				peek(responseNetwork_in, ResponseMsg, block_on="Addr") {
					// Making sure directory is sending to data to the right guy
      	  DPRINTF(RubySlicc, "Response address:%s \n",in_msg.Addr);
					//assert(in_msg.Destination.isElement(machineID));
	
					Entry cache_entry := getL1CacheEntry(in_msg.Addr);
					TBE tbe := TBEs[in_msg.Addr];

					// Check if its data
					if (in_msg.Type == CoherenceResponseType:DATA) {
						DPRINTF(RubySlicc, "DATA SENT TYPE FROM DIR\n ");
						trigger(Event:Data, in_msg.Addr, cache_entry, tbe);
					}	
					if (in_msg.Type == CoherenceResponseType:EDATA) {
						DPRINTF(RubySlicc, "EDATA sent from dir\n");
						trigger(Event:EData, in_msg.Addr, cache_entry, tbe);
					}
					if (in_msg.Type == CoherenceResponseType:DATA_TO_CACHE) {
						DPRINTF(RubySlicc, "DATA SENT TYPE FROM CACHE\n");
						trigger(Event:Data, in_msg.Addr, cache_entry, tbe);
					}
					if (in_msg.Type == CoherenceResponseType:MEMORY_ACK) {
						DPRINTF(RubySlicc, "Memory ack from dir\n");
						trigger(Event:Ack, in_msg.Addr, cache_entry, tbe);
					}
				}
		//	}
		}
	}

	in_port(requestNetworkWB_in, RequestMsg, requestToCacheWB, rank=0) {
		if (requestNetworkWB_in.isReady()) {
			//if (is_blocked == false) {
				peek(requestNetworkWB_in, RequestMsg, block_on="Addr") {
					Entry cache_entry :=getL1CacheEntry(in_msg.Addr);
					TBE tbe := TBEs[in_msg.Addr];

				// This assert should not cause problems, as a broadcast call should put all the machineIDs in the Destination
    	   	DPRINTF(RubySlicc, "address: %s, requestor: %s, Type: %s\n",in_msg.Addr, in_msg.Requestor, in_msg.Type);
	
					assert(in_msg.Destination.isElement(machineID));
					// Check if the request type is GETS or GETI
					if (in_msg.Type == CoherenceRequestType:GETS) {
						// Check if requestor is receiver
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GETS
							trigger(Event:OWN_GETS, in_msg.Addr, cache_entry, tbe);
						}
						else {	
							if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
								if (cache_entry.finalDestinationSet.isEmpty()) {
									cache_entry.finalDestinationSet.add(in_msg.Requestor);
									DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
								}
							}
							if (is_valid(cache_entry)) {
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, cache_entry.isAtomic);
							}
							if (is_valid(tbe)) {	
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, tbe.isAtomic);
							}
	
							if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s\n", in_msg.Addr);
								DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
							}
							else {
								trigger(Event:Other_GETS, in_msg.Addr, cache_entry, tbe);
							}
						}
					}
	
					else if (in_msg.Type == CoherenceRequestType:INV) {
						trigger(Event:INV, in_msg.Addr, cache_entry, tbe);
					}

					else if (in_msg.Type == CoherenceRequestType:GETI) {
						// Check if requestor is receiever
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GETI
							trigger(Event:OWN_GETI, in_msg.Addr, cache_entry, tbe);
						}
						else {
							trigger(Event:Other_GETI, in_msg.Addr, cache_entry, tbe);
						}
					}		
					else if (in_msg.Type == CoherenceRequestType:GETM) {
						// Check if requestor is receiver
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GetM
							trigger(Event:OWN_GETM, in_msg.Addr, cache_entry, tbe);
						}
						else {
							DPRINTF(RubySlicc, "OTHER GETM requestor %s\n", in_msg.Requestor);
				
							if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
								if (cache_entry.finalDestinationSet.isEmpty()) {
									cache_entry.finalDestinationSet.add(in_msg.Requestor);
									DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
								}
							}
							if (is_valid(cache_entry)) {
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, cache_entry.isAtomic);
							}
							if (is_valid(tbe)) {	
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, tbe.isAtomic);
							}
	
							if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s", in_msg.Addr);
								DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
							}
							else {
								trigger(Event:Other_GETM, in_msg.Addr, cache_entry, tbe);
							}
						}
					}
					else if (in_msg.Type == CoherenceRequestType:GETMA) {
						// Check if requestor is receiver
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GetM
							trigger(Event:OWN_GETM, in_msg.Addr, cache_entry, tbe);
						}
						else {
							DPRINTF(RubySlicc, "OTHER GETM requestor %s\n", in_msg.Requestor);
				
							if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
								if (cache_entry.finalDestinationSet.isEmpty()) {
									cache_entry.finalDestinationSet.add(in_msg.Requestor);
									DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
								}
							}
							if (is_valid(cache_entry)) {
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, cache_entry.isAtomic);
							}
							if (is_valid(tbe)) {	
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, tbe.isAtomic);
							}
	
							if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s", in_msg.Addr);
								DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
							}
							else {
								trigger(Event:Other_GETM_Approx, in_msg.Addr, cache_entry, tbe);
							}
						}
					}
	
	
					else if (in_msg.Type == CoherenceRequestType:GETX) {
						if (in_msg.Requestor == machineID) {
							trigger(Event:OWN_GETM, in_msg.Addr, cache_entry, tbe);
						}
						else {
							if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
								if (cache_entry.finalDestinationSet.isEmpty()) {
									cache_entry.finalDestinationSet.add(in_msg.Requestor);
									DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
								}
							}
							if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s", in_msg.Addr);
								DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
							}
							else {
								trigger(Event:Other_GETM, in_msg.Addr, cache_entry, tbe);
							}
						}
					}
					else if (in_msg.Type == CoherenceRequestType:PUTM) {
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GETI
							trigger(Event:OWN_PUTM, in_msg.Addr, cache_entry, tbe);
						}
						else {
							trigger(Event:Other_PUTM, in_msg.Addr, cache_entry, tbe);
						}
					}
				}
			//}
		}
	}

	in_port(requestNetwork_in, RequestMsg, requestToCache, rank=0) {
		if (requestNetwork_in.isReady()) {
			//if (is_blocked == false) {
				peek(requestNetwork_in, RequestMsg, block_on="Addr") {
					Entry cache_entry :=getL1CacheEntry(in_msg.Addr);
					TBE tbe := TBEs[in_msg.Addr];

				// This assert should not cause problems, as a broadcast call should put all the machineIDs in the Destination
    	   	DPRINTF(RubySlicc, "address: %s, requestor: %s, Type: %s\n",in_msg.Addr, in_msg.Requestor, in_msg.Type);
	
					assert(in_msg.Destination.isElement(machineID));
					// Check if the request type is GETS or GETI
					if (in_msg.Type == CoherenceRequestType:GETS) {
						// Check if requestor is receiver
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GETS
							trigger(Event:OWN_GETS, in_msg.Addr, cache_entry, tbe);
						}
						else {	
							if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
								if (cache_entry.finalDestinationSet.isEmpty()) {
									cache_entry.finalDestinationSet.add(in_msg.Requestor);
									DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
								}
							}
							if (is_valid(cache_entry)) {
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, cache_entry.isAtomic);
							}
							if (is_valid(tbe)) {	
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, tbe.isAtomic);
							}
	
							if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s\n", in_msg.Addr);
								DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
							}
							else {
								trigger(Event:Other_GETS, in_msg.Addr, cache_entry, tbe);
							}
						}
					}
	
					else if (in_msg.Type == CoherenceRequestType:INV) {
						trigger(Event:INV, in_msg.Addr, cache_entry, tbe);
					}

					else if (in_msg.Type == CoherenceRequestType:GETI) {
						// Check if requestor is receiever
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GETI
							trigger(Event:OWN_GETI, in_msg.Addr, cache_entry, tbe);
						}
						else {
							trigger(Event:Other_GETI, in_msg.Addr, cache_entry, tbe);
						}
					}
					else if (in_msg.Type == CoherenceRequestType:GETM) {
						// Check if requestor is receiver
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GetM
							trigger(Event:OWN_GETM, in_msg.Addr, cache_entry, tbe);
						}
						else {
							DPRINTF(RubySlicc, "OTHER GETM requestor %s\n", in_msg.Requestor);
				
							if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
								if (cache_entry.finalDestinationSet.isEmpty()) {
									cache_entry.finalDestinationSet.add(in_msg.Requestor);
									DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
								}
							}
							if (is_valid(cache_entry)) {
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, cache_entry.isAtomic);
							}
							if (is_valid(tbe)) {	
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, tbe.isAtomic);
							}
	
							if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s", in_msg.Addr);
								DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
							}
							else {
								trigger(Event:Other_GETM, in_msg.Addr, cache_entry, tbe);
							}
						}
					}
					else if (in_msg.Type == CoherenceRequestType:GETMA) {
						// Check if requestor is receiver
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GetM
							trigger(Event:OWN_GETM, in_msg.Addr, cache_entry, tbe);
						}
						else {
							DPRINTF(RubySlicc, "OTHER GETM requestor %s\n", in_msg.Requestor);
				
							if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
								if (cache_entry.finalDestinationSet.isEmpty()) {
									cache_entry.finalDestinationSet.add(in_msg.Requestor);
									DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
								}
							}
							if (is_valid(cache_entry)) {
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, cache_entry.isAtomic);
							}
							if (is_valid(tbe)) {	
								DPRINTF(RubySlicc, "ADDRESS: %s, isATOMIC: %s\n", in_msg.Addr, tbe.isAtomic);
							}
	
							if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s", in_msg.Addr);
								DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
							}
							else {
								trigger(Event:Other_GETM_Approx, in_msg.Addr, cache_entry, tbe);
							}
						}
					}
	
	
					else if (in_msg.Type == CoherenceRequestType:GETX) {
						if (in_msg.Requestor == machineID) {
							trigger(Event:OWN_GETM, in_msg.Addr, cache_entry, tbe);
						}
						else {
							if (is_dataSendState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Before final destination set :%s\n", cache_entry.finalDestinationSet);
								if (cache_entry.finalDestinationSet.isEmpty()) {
									cache_entry.finalDestinationSet.add(in_msg.Requestor);
									DPRINTF(RubySlicc, "After final destination set :%s\n", cache_entry.finalDestinationSet);
								}
							}
							if (isAtomic(in_msg.Addr) && is_dataWaitingState(tbe, in_msg.Addr, cache_entry)) {
								DPRINTF(RubySlicc, "Current address is atomic. Avoid othe requests: %s", in_msg.Addr);
								DPRINTF(RubySlicc, "Is this data waiting state: %s\n", is_dataWaitingState(tbe, in_msg.Addr, cache_entry));
							}
							else {
								trigger(Event:Other_GETM, in_msg.Addr, cache_entry, tbe);
							}
						}
					}
					else if (in_msg.Type == CoherenceRequestType:PUTM) {
						if (in_msg.Requestor == machineID) {
							// Requestor is receiver. Seeing its own GETI
							trigger(Event:OWN_PUTM, in_msg.Addr, cache_entry, tbe);
						}
						else {
							trigger(Event:Other_PUTM, in_msg.Addr, cache_entry, tbe);
						}
					}
				}
			//}
		}
	}


	in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...", rank=0) {
		if (mandatoryQueue_in.isReady()) {
			if (is_blocked == 0) {
				peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {
					Entry cache_entry := getL1CacheEntry(in_msg.LineAddress);
					DPRINTF(RubySlicc, "MSG ADDR: %s, MSG TYPE: %s is_invalid: %s, cache avail: %s\n",in_msg.LineAddress, in_msg.Type, is_invalid(cache_entry), Dcache.cacheAvail(in_msg.LineAddress));
					 //Checking what data is associated with Store
	
					if (in_msg.Type == RubyRequestType:IFETCH) {
						if (is_invalid(cache_entry) && Icache.cacheAvail(in_msg.LineAddress) == false) {
							trigger(Event:Replacement, Icache.cacheProbe(in_msg.LineAddress), 
											getL1CacheEntry(Icache.cacheProbe(in_msg.LineAddress)), 
											TBEs[Icache.cacheProbe(in_msg.LineAddress)]);
						}
						else {
							trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, 
										cache_entry, TBEs[in_msg.LineAddress]);
						}
					}
					else {
						if (is_invalid(cache_entry) && Dcache.cacheAvail(in_msg.LineAddress) == false) {
							trigger(Event:Replacement, Dcache.cacheProbe(in_msg.LineAddress), 
											getL1CacheEntry(Dcache.cacheProbe(in_msg.LineAddress)),
											TBEs[Dcache.cacheProbe(in_msg.LineAddress)]);
						}
						else {
							trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, cache_entry, TBEs[in_msg.LineAddress]);
						}
					}
				}
			}
			else {
				 //Allow RMW writes to go ahead
				peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {				
					Entry cache_entry := getL1CacheEntry(in_msg.LineAddress);
					DPRINTF(RubySlicc, "MSG ADDR: %s, MSG TYPE: %s is_invalid: %s, cache avail: %s\n",in_msg.LineAddress, in_msg.Type, is_invalid(cache_entry), Dcache.cacheAvail(in_msg.LineAddress));
					if (in_msg.Type == RubyRequestType:Locked_RMW_Write) {					
						trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, cache_entry, TBEs[in_msg.LineAddress]);
					}
				}
			}
		}
	}

	
	/* For ruby random tester */
	/*
	in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...", rank=0) {
		if (mandatoryQueue_in.isReady()) { 
			if (is_blocked == false) {
				peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {
					Entry cache_entry := getL1CacheEntry(in_msg.LineAddress);

					DPRINTF(RubySlicc, "MSG ADDR: %s, MSG TYPE: %s is_invalid: %s, cache avail: %s\n",in_msg.LineAddress, in_msg.Type, is_invalid(cache_entry), Dcache.cacheAvail(in_msg.LineAddress));					 
					if (is_invalid(cache_entry) && Dcache.cacheAvail(in_msg.LineAddress) == false) {
						trigger(Event:Replacement, Dcache.cacheProbe(in_msg.LineAddress), 
										getL1CacheEntry(Dcache.cacheProbe(in_msg.LineAddress)),
										TBEs[Dcache.cacheProbe(in_msg.LineAddress)]);
					}
					else {
						trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, cache_entry, TBEs[in_msg.LineAddress]);
					}
					
				}
			}
			else {
				 //Allow RMW writes to go ahead
				peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {				
					Entry cache_entry := getL1CacheEntry(in_msg.LineAddress);
					DPRINTF(RubySlicc, "MSG ADDR: %s, MSG TYPE: %s is_invalid: %s, cache avail: %s\n",in_msg.LineAddress, in_msg.Type, is_invalid(cache_entry), Dcache.cacheAvail(in_msg.LineAddress));
					if (in_msg.Type == RubyRequestType:Locked_RMW_Write) {					
						trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress, cache_entry, TBEs[in_msg.LineAddress]);
					}
				}
			}
		}
	}
	*/
		
  // ACTIONS
  action(as_issueGETSMem, "as", desc="Issue GETS") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
        out_msg.Addr := address;
        out_msg.Type := CoherenceRequestType:GETS;
        out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
        DPRINTF(RubySlicc, "ISSUE GETS address: %s, destination: %s\n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
      }
    }
  }

 
  action(ai_issueGETINSTR, "ai", desc="Issue GETINSTR") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
        out_msg.Addr := address;
        out_msg.Type := CoherenceRequestType:GETI;
        out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
        DPRINTF(RubySlicc, "ISSUE GETINSTR address: %s, destination: %s\n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;

      }
    }
  }

	action(bm_issueGETMR, "bmr", desc="Issue GETM from request queue") {		
  	peek(requestNetwork_in, RequestMsg) {
			enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETM;
				out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));				
				out_msg.Destination.makeSidePacket();
				//out_msg.Destination.broadcast(MachineType:MemCache);
        DPRINTF(RubySlicc, "ISSUE GETM address: %s, destination: %s \n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
			}
		}
	}


	action(bm_issueUpg, "bupg", desc="Issue UPG") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {			
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:UPG;
				out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));				
				out_msg.Data := in_msg.Data64Bit;
				out_msg.DataOffset := in_msg.Offset;
				out_msg.DataSize := in_msg.Size;
				out_msg.Destination.makeSpecial();
        DPRINTF(RubySlicc, "ISSUE UPG address: %s, destination: %s \n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
			}
		}
	}

	action(bam_issueGETM, "bama", desc="Issue GETM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETM;
				out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));


				out_msg.Data := in_msg.Data64Bit;
				out_msg.DataOffset := in_msg.Offset;
				out_msg.DataSize := in_msg.Size;
        DPRINTF(RubySlicc, "ISSUE GETM address: %s, destination: %s \n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;

			}
		}
	}


	action(bm_issueGETM, "bm", desc="Issue GETM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(requestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETM;
				out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
				//out_msg.Destination.broadcast(MachineType:MemCache);

				out_msg.Data := in_msg.Data64Bit;
				out_msg.DataOffset := in_msg.Offset;
				out_msg.DataSize := in_msg.Size;
        DPRINTF(RubySlicc, "ISSUE GETM address: %s, destination: %s \n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
			}
		}
	}

	action(bam_issueGETMATOMICST, "bmst", desc="Issue GETM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(atomicRequestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETMATOMICST;
				out_msg.Requestor := machineID;
				out_msg.Destination.broadcast(MachineType:L1Cache);				
				out_msg.Destination.remove(machineID);
        DPRINTF(RubySlicc, "ISSUE GETMATOMIC address: %s, destination: %s \n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control; 
			}
		}
	}

	action(bam_issueGETMATOMICEN, "bmen", desc="Issue GETM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(atomicRequestNetwork_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:GETMATOMICEN;
				out_msg.Requestor := machineID;				
				out_msg.Destination.broadcast(MachineType:L1Cache);				
				out_msg.Destination.remove(machineID);
        DPRINTF(RubySlicc, "ISSUE GETMATOMIC address: %s, destination: %s \n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control; 
			}
		}
	}


	action(bpm_issuePUTM, "bpm", desc="Issue PUTM") {
		peek(mandatoryQueue_in, RubyRequest) {
			enqueue(requestNetworkWB_out, RequestMsg, l1_request_latency) {			
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:PUTM;
				out_msg.Requestor := machineID;
				// Need a broadcast actually
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
				DPRINTF(RubySlicc, "ISSUE PUTM address: %s, destination: %s\n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
				out_msg.DataBlk := cache_entry.DataBlk;

			}
		}
	}

	action(bpm_issuePUTMR, "bpmr", desc="Issue BPM from request queue") {
  	peek(requestNetwork_in, RequestMsg) {
			enqueue(requestNetworkWB_out, RequestMsg, l1_request_latency) {
				out_msg.Addr := address;
				out_msg.Type := CoherenceRequestType:PUTM;
				out_msg.Requestor := machineID;
				// Need a broadcast actually
				out_msg.Destination.broadcast(MachineType:L1Cache);
				out_msg.Destination.add(map_Address_to_Directory(address));
				DPRINTF(RubySlicc, "ISSUE PUTM address: %s, destination: %s\n",address, out_msg.Destination);
        out_msg.MessageSize := MessageSizeType:Control;
				out_msg.DataBlk := cache_entry.DataBlk;
			}
		}
	}

	action(bpm_issuePUTMD, "bpmD", desc="Issue BPM from request queue") {
		enqueue(requestNetworkWB_out, RequestMsg, l1_request_latency) {
			out_msg.Addr := address;
			out_msg.Type := CoherenceRequestType:PUTM;
			out_msg.Requestor := machineID;
			// Need a broadcast actually
			out_msg.Destination.broadcast(MachineType:L1Cache);
			out_msg.Destination.add(map_Address_to_Directory(address));
			DPRINTF(RubySlicc, "ISSUE PUTM address: %s, destination: %s\n",address, out_msg.Destination);
       out_msg.MessageSize := MessageSizeType:Control;
			out_msg.DataBlk := cache_entry.DataBlk;
		}
	}


	action(send_NoData ,"nd", desc="Send no data to directory") {
		enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
			out_msg.Addr := address;
			out_msg.Type := CoherenceResponseType:NO_DATA;
			out_msg.Sender := machineID;
			out_msg.Destination.add(map_Address_to_Directory(address));
			out_msg.MessageSize := MessageSizeType:Control;
		}
	}


	action(ct_sendDataTBEToDir, "ct", desc="Send data from TBE to directory") {
		enqueue(responseNetwork_out, ResponseMsg, l1_response_latency) {
			assert(is_valid(tbe));
			out_msg.Addr := address;
			out_msg.Type := CoherenceResponseType:DATA_TO_WB;
			out_msg.Sender := machineID;			
			out_msg.Destination.add(map_Address_to_Directory(address));
			DPRINTF(RubySlicc, "SEND DATA FROM TBE TO DIR address: %s, destination: %s datablk %s\n",address, out_msg.Destination, tbe.DataBlk);
      out_msg.MessageSize := MessageSizeType:Control;
			out_msg.DataBlk := tbe.DataBlk;
     
		}
	}

	
  action(r_load_hit, "r",
         desc="new data from mem, notify sequencer the load completed.")
  {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "LOAD HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    sequencer.readCallback(address, cache_entry.DataBlk, false);
  }

  action(rx_load_hit, "rx",
         desc="data already present, notify sequencer the load completed.")
  {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "CACHED LOAD HIR Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    sequencer.readCallback(address, cache_entry.DataBlk, true);
  }

  action(s_store_hit, "s",
         desc="If not prefetch, notify sequencer that store completed.")
  {
    assert(is_valid(cache_entry));   
    sequencer.writeCallback(address, cache_entry.DataBlk);
    DPRINTF(RubySlicc, "STORE HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    cache_entry.Dirty := true;
  }

  action(sx_store_hit, "sx",
         desc="If not prefetch, notify sequencer that store completed.")
  {
    assert(is_valid(cache_entry));
    sequencer.writeCallback(address, cache_entry.DataBlk, true);
    DPRINTF(RubySlicc, "CACHED STORE HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    cache_entry.Dirty := true;		
  }


  action(sx_atomic_store_hit_set, "sax",
         desc="If not prefetch, notify sequencer that store completed.")
  {
    assert(is_valid(cache_entry));
    sequencer.writeCallback(address, cache_entry.DataBlk, true);
    DPRINTF(RubySlicc, "CACHED STORE HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    cache_entry.Dirty := true;
		cache_entry.isAtomic := true;
  }
 
 action(sx_atomic_store_hit_unset, "saxu",
         desc="If not prefetch, notify sequencer that store completed.")
  {
    assert(is_valid(cache_entry));
    sequencer.writeCallback(address, cache_entry.DataBlk, true);
    DPRINTF(RubySlicc, "CACHED STORE HIT Addr:%s, DataBlk:%s\n", address, cache_entry.DataBlk);
    cache_entry.Dirty := true;
		cache_entry.isAtomic := false;
  }

  action(i_allocateAtomicTBE, "iato", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(TBEs);
    assert(is_valid(cache_entry));
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
		DPRINTF(RubySlicc, "Putting TBE DataBlk for replacement:%s\n", tbe.DataBlk);
		tbe.isAtomic := true;
  }


  action(i_allocateTBE, "i", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(TBEs);
    assert(is_valid(cache_entry));
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
		DPRINTF(RubySlicc, "Putting TBE DataBlk for replacement:%s\n", tbe.DataBlk);
		tbe.isAtomic := false;
  }

  action(k_popMandatoryQueue, "k", desc="Pop mandatory queue.") {
    mandatoryQueue_in.dequeue();
  }

  action(l_popRequestWBQueue, "lwb",desc="Pop incoming request queue and profile the delay within this virtual network") {
    profileMsgDelay(2, requestNetworkWB_in.dequeue());
  }

  action(l_popRequestQueue, "l",desc="Pop incoming request queue and profile the delay within this virtual network") {
    profileMsgDelay(2, requestNetwork_in.dequeue());
  }

 action(l_popAtomicRequestQueue, "lat",
    desc="Pop incoming request queue and profile the delay within this virtual network") {
    profileMsgDelay(2, atomicRequestNetwork_in.dequeue());
  }

  action(o_popIncomingResponseQueue, "o",
    desc="Pop Incoming Response queue and profile the delay within this virtual network") {
    profileMsgDelay(1, responseNetwork_in.dequeue());
  }

  action(s_deallocateTBE, "sd", desc="Deallocate TBE") {
    // Before unsettng, calculate the inter-core coherence latency
		TBEs.deallocate(address);
    unset_tbe();
  }

  action(u_writeDataToL1Cache, "u", desc="Write data to cache") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
			DPRINTF(RubySlicc, "DataBlk write: %s\n", in_msg.DataBlk);
      cache_entry.DataBlk := in_msg.DataBlk;
      cache_entry.Dirty := in_msg.Dirty;
			DPRINTF(RubySlicc, "Send time: %s, first scheduled time: %s, address: %s\n", in_msg.SendTime, tbe.firstScheduledTime, address);
			tbe.interCoreCohLatency := in_msg.SendTime - tbe.firstScheduledTime;
			tbe.CoreID := in_msg.Destination.smallestNodeID();

			tbe.arbitLatency := getArbitLatency(tbe.startTime, tbe.CoreID, tbe.zeroArb);
    }
  }


  action(ff_deallocateL1CacheBlock, "\f", desc="Deallocate L1 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    if (Dcache.isTagPresent(address)) {
      Dcache.deallocate(address);
    }
		else {
      Icache.deallocate(address);
    }
    unset_cache_entry();
  }

  action(oo_allocateAtomicL1DCacheBlock, "\o", desc="Set L1 D-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(Dcache.allocate(address, new Entry));
    }

		DPRINTF(RubySlicc, "Atomic allocate\n");
		DPRINTF(RubySlicc, "IS VALID: %s", is_valid(cache_entry));
		cache_entry.isAtomic := true;
  }

  action(oo_allocateL1DCacheBlock, "\oatom", desc="Set L1 D-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(Dcache.allocate(address, new Entry));
    }

  }

  action(pp_allocateL1ICacheBlock, "\p", desc="Set L1 I-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(Icache.allocate(address, new Entry));
    }
  }	

  action(z_stallAndWaitMandatoryQueue, "\z", desc="recycle L1 request queue") {
		DPRINTF(RubySlicc, "Stalling address: %s\n", address);
    stall_and_wait(mandatoryQueue_in, address);
  }

	action(z_stallAndWaitRequestQueue, "\zresp", desc="recycle L1 response queue") {
		DPRINTF(RubySlicc, "Stalling response queue: %s\n", address);
		stall_and_wait(requestNetwork_in, address);		
	}


  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
		if (isAtomic(address)) {
			DPRINTF(RubySlicc, "Cannot wake up dependents. Atomic access blocked: %s\n", address);
		}
		else {
    	wakeUpBuffers(address);
		}
  }

	action(ka_wakeUpAllDependents, "ka", desc="wake-up all dependents") {		
		if (isAtomic(address)) {
			DPRINTF(RubySlicc, "Cannot wake up dependents. Atomic access blocked: %s\n", address);
		}
		else {
    	wakeUpAllBuffers();
		}
	} 

	action(setBlocked, "\sb", desc="Block controller") {
		is_blocked := is_blocked + 1;
	}

	action(setUnblocked, "\sub", desc="Unblock controller") {
		is_blocked := is_blocked - 1;
	}


  //*****************************************************
  // TRANSITIONS
  //*****************************************************

	// Transition from I to IS_AD
	transition(I, Load, IS_AD) {	
		oo_allocateL1DCacheBlock;	
		i_allocateTBE;
		as_issueGETSMem;
		k_popMandatoryQueue;
	}

	transition(I, Ifetch, fIS_AD) {		
		pp_allocateL1ICacheBlock;	
		i_allocateTBE;
		ai_issueGETINSTR;
		k_popMandatoryQueue;
	}

	// Transition from IS_AD to IS_D
	transition(IS_AD, {OWN_GETS, OWN_GETI}, IS_D) {
		l_popRequestQueue;
	}

	transition(fIS_AD, {OWN_GETS, OWN_GETI}, fIS_D) {
		l_popRequestQueue;
	}

	transition(IS_D, EData, E) {
		u_writeDataToL1Cache;
		r_load_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}

	transition(fIS_D, {EData, Data}, S) {
		u_writeDataToL1Cache;
		r_load_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}

	transition(IS_D, Data, S) {
		u_writeDataToL1Cache;
		r_load_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}

	// Transition from I to IM_AD
	transition(I, Store, IM_AD) {
		oo_allocateL1DCacheBlock;
		i_allocateTBE;
		bm_issueGETM;
		k_popMandatoryQueue;
	}

	// Transition from IM_AD to IM_D
	transition(IM_AD, OWN_GETM, IM_D) {
		l_popRequestQueue;
	}

	// Transition from IM_D to M
	transition(IM_D, {Data,EData}, M) {
		u_writeDataToL1Cache;
		s_store_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}

	transition({IM_D,IM_DI, IM_DS}, OWN_GETM) {
		l_popRequestQueue;
	}

	// Transition from IS_D to IS_DI
	transition(IS_D, {Other_GETM}, IS_DI) {
		l_popRequestQueue;
	}

	transition(IS_DI, Other_GETM) {
		l_popRequestQueue;
	}

	transition(IS_DI, EData, EI_A) {
		u_writeDataToL1Cache;
		r_load_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		bpm_issuePUTMD;
	}


	// Transition from IS_DI to I
	transition(IS_DI, {Data}, I) {
		u_writeDataToL1Cache;
		r_load_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
    ff_deallocateL1CacheBlock;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}
	
	// Transition from IM_D to IM_DS
	transition(IM_D, {Other_GETS,Other_GETI}, IM_DS) {
		l_popRequestQueue;
	}

	// Transition from IM_DS to S
	transition({SM_DS,IM_DS}, {EData,Data}, MS_A) {
		u_writeDataToL1Cache;
		s_deallocateTBE;
		s_store_hit;		
		bpm_issuePUTMD;
		o_popIncomingResponseQueue;		
	}

	// Transition from IM_D to IM_DI
	transition(IM_D, {Other_GETM}, IM_DI) {
		l_popRequestQueue;
	}


	// Transition from IM_DI to I
	transition({SM_DI,IM_DI}, {EData,Data}, MI_A) {
		u_writeDataToL1Cache;
		s_deallocateTBE;
		s_store_hit;
		bpm_issuePUTMD;
		o_popIncomingResponseQueue;		
	}


	// Transition from IM_DS to IM_DSI
	transition(IM_DS,{Other_GETM}, IM_DI) {
		l_popRequestQueue;
	}


	transition({IM_DS,SM_DS}, Other_GETS) {
		l_popRequestQueue;
	}
	
	transition(S, Other_GETM, I) {
		ff_deallocateL1CacheBlock;
		l_popRequestQueue;
	}

	transition(S, Store, SM_AD) {
		bm_issueGETM;
		i_allocateTBE;
		k_popMandatoryQueue;
	}

	transition(SM_AD, OWN_GETM, SM_D) {
		l_popRequestQueue;
	}

	transition(SM_AD, {Other_GETS, Other_GETI}) {
		l_popRequestQueue;
	}

	transition(SM_AD, Other_GETM, IM_AD) {
		l_popRequestQueue;
	}

	transition(SM_D, {Other_GETI, Other_GETS}, SM_DS) {
		l_popRequestQueue;
	}


	transition(SM_D, Other_GETM, SM_DI) {
		l_popRequestQueue;
	}

	transition(SM_DS, OWN_GETM, SM_DI) {
		l_popRequestQueue;
	}	
	

	transition({E,EI_A, ES_A, MS_A}, Load) {
		rx_load_hit;
		k_popMandatoryQueue;	
	}

	transition(E, Other_GETM, EI_A) {
		bpm_issuePUTMR; 
		l_popRequestQueue;
	}

	transition(E, {Other_GETS, Other_GETI}, ES_A) {
		bpm_issuePUTMR;
		l_popRequestQueue;
	}

	transition(E, Store, M) {
		sx_store_hit;
		k_popMandatoryQueue;
	}	

	// Transition from SM_D to M
	transition(SM_D, {EData,Data}, M) {	
		u_writeDataToL1Cache;
		sx_store_hit;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
		kd_wakeUpDependents;
		ka_wakeUpAllDependents;
	}
	
	
		// Transition from M to I
	transition (M, {Other_GETM}, MI_A) {	
		bpm_issuePUTMR; 
		l_popRequestQueue;
	}

	transition(M, {Other_GETI, Other_GETS}, MS_A) {
		bpm_issuePUTMR;
		l_popRequestQueue;
	}
	
	transition(E, Replacement, EI_A) {
		bpm_issuePUTM;
		z_stallAndWaitMandatoryQueue;
	}

	transition(EI_A, {Other_GETS, Other_GETM, Other_GETI}) {
		l_popRequestQueue;	
	}

	transition(ES_A, {Other_GETS, Other_GETI}) {
		l_popRequestQueue;
	}

	transition(ES_A, {Other_GETM}, EI_A) {
		l_popRequestQueue;
	}

	transition(EI_A, OWN_PUTM, I) {
		send_NoData;
		ff_deallocateL1CacheBlock;
		l_popRequestWBQueue;
		ka_wakeUpAllDependents;
		kd_wakeUpDependents;
	}

	transition(ES_A, OWN_PUTM, S) {
		send_NoData;
		ff_deallocateL1CacheBlock;
		l_popRequestWBQueue;
		ka_wakeUpAllDependents;
		kd_wakeUpDependents;
	}
	

	// Transition from M to MI_A
	transition(M, Replacement, MI_A) {
		bpm_issuePUTM;
		z_stallAndWaitMandatoryQueue;
	}

	transition({MI_A,MS_A, EI_A, ES_A}, Replacement) {
		z_stallAndWaitMandatoryQueue;
	}

	transition(S, {INV,Replacement}, I) {
		ff_deallocateL1CacheBlock;		
	}

	// Transition from MI_A to I
	transition(MI_A, OWN_PUTM, I) {
		i_allocateTBE;
		ct_sendDataTBEToDir;
		ff_deallocateL1CacheBlock;
		l_popRequestWBQueue;
		ka_wakeUpAllDependents;
		kd_wakeUpDependents;
		s_deallocateTBE;
	}

	transition(MS_A, OWN_PUTM, S) {
		i_allocateTBE;
		ct_sendDataTBEToDir;
		l_popRequestWBQueue;
		ka_wakeUpAllDependents;
		kd_wakeUpDependents;
		s_deallocateTBE;
	}

	transition(MI_A, {Other_GETS, Other_GETI, Other_GETM}) {
		l_popRequestQueue;
	}

	transition(MS_A, {Other_GETM}, MI_A) {
		l_popRequestQueue;
	}

	transition(MS_A, {Other_GETS, Other_GETI}) {
		l_popRequestQueue;
	}

	transition(I, {Other_GETM}) {
		l_popRequestQueue;
	}

	transition(I, {Other_GETS, Other_GETI}) {
		l_popRequestQueue;
	}

	transition({S,IS_D, IS_DI}, {Other_GETI, Other_GETS}) {
		l_popRequestQueue;
	}


	// Stall transitions
	transition({fIS_AD, fIS_D, IS_AD, IS_D, IS_DI, IM_AD, IM_D, IM_DI, IM_DS}, {Load, Ifetch, Store, Replacement, RMW_Read, RMW_Write}) {
		z_stallAndWaitMandatoryQueue;
	}

	transition({fIS_AD, IS_AD, fIS_D, IM_AD, IM_AL}, {Other_GETS, Other_GETI}) {
		l_popRequestQueue;
	}

	transition({fIS_AD, IS_AD, IM_AD, IM_AL, IM_D, IS_D, IM_DI, IM_DS, IS_DI, I, S}, Other_PUTM) {
		l_popRequestWBQueue;
	}

	transition({fIS_AD, IS_AD, IM_AD, IM_AL}, {Other_GETM}) {
		l_popRequestQueue;
	}


	transition({IM_DI, SM_DI, IM_WL, SM_WL}, {Other_GETM, Other_GETS, Other_GETI}) {
		l_popRequestQueue;
	}

	transition({S, SM_DS, SM_AD, SM_D, SM_DI, M, MI_A}, Load) {
		rx_load_hit;
		k_popMandatoryQueue;	
	}


	transition({S, SM_DS, SM_AD, SM_D, SM_DI, M, MI_A, E, EI_A, ES_A}, {Ifetch}) {
		rx_load_hit;
		k_popMandatoryQueue;	
	}

	transition({SM_AD, SM_D, SM_DI, SM_DS}, {Replacement, Store}) {
		z_stallAndWaitMandatoryQueue;
	}

	transition({M, MI_A}, Store) {
		sx_store_hit;
		k_popMandatoryQueue;
	}

	transition({EI_A,ES_A, MS_A}, Store, MI_A) {
		sx_store_hit;
		k_popMandatoryQueue;
	}

	transition({MI_A,MS_A, EI_A, ES_A}, RMW_Read) {
		z_stallAndWaitMandatoryQueue;
	}


////////////////////////////////////


	// RMW RMR
	transition(M, RMW_Read, M_L) {
		bam_issueGETMATOMICST;
		sx_atomic_store_hit_set;
		k_popMandatoryQueue;
	}

	transition(I, RMW_Read, IM_ADL) {
		oo_allocateAtomicL1DCacheBlock;
		i_allocateAtomicTBE;
		bam_issueGETM;
		bam_issueGETMATOMICST;
		k_popMandatoryQueue;
	}

	transition(IM_ADL, OWN_GETM, IM_DL) {
		l_popRequestQueue;
	}

	//Transition from IM_AD to IM_A
	transition(IM_ADL, Data, IM_AL) {
		u_writeDataToL1Cache;
		s_deallocateTBE;
		o_popIncomingResponseQueue;
	}

	// Transition from IM_A to M
	transition(IM_AL, OWN_GETM, M_L) {		
		sx_atomic_store_hit_set;
		l_popRequestQueue;		
	}

	transition({S}, RMW_Read, SM_WL) {		
		bm_issueUpg;
		bam_issueGETMATOMICST;
		k_popMandatoryQueue;
	}

	transition(E, RMW_Read, M_L) {
		bam_issueGETMATOMICST;
		sx_atomic_store_hit_set;
		k_popMandatoryQueue;
	}

	transition(SM_WL, OWN_GETM, M_L) {
		sx_atomic_store_hit_set;
		l_popRequestQueue;
	}
	
	transition({SM_WL,IM_WL}, Other_PUTM, IM_WL) {
		l_popRequestWBQueue;
	}


	transition(IM_WL, OWN_GETM, IM_DL) {
		bm_issueGETMR;
		i_allocateTBE;
		l_popRequestQueue;
	}

	transition(IM_DL, OWN_GETM) {
		l_popRequestQueue;
	}

	transition(IM_ADL, {Other_GETS, Other_GETM}) {
		l_popRequestQueue;
	}

	transition({IM_ADL,IM_DL,M_L}, {Other_PUTM}) {
		l_popRequestWBQueue;
	}

	transition({IM_DL,M_L}, {Other_GETS, Other_GETM}) {
		z_stallAndWaitRequestQueue;
	}

	transition({IM_ADL, IM_DL, SM_WL, M_L}, {Load, Store, Replacement, RMW_Read}) {	
		z_stallAndWaitMandatoryQueue;
	}

	transition(IM_DL, {EData,Data}, M_L) {
		u_writeDataToL1Cache;
		sx_atomic_store_hit_set;
		s_deallocateTBE;
		o_popIncomingResponseQueue;		
	}

	transition(M_L, RMW_Write, M) {
		bam_issueGETMATOMICEN;		
		sx_atomic_store_hit_unset;
		k_popMandatoryQueue;
		ka_wakeUpAllDependents;
		kd_wakeUpDependents;
	}

//////////////////////////////////////

	transition({I, M, S, fIS_AD, fIS_D, IS_AD, IS_D, IS_DI, IM_AD, IM_D,  IM_DS, IM_DI, SM_AD,  SM_D, SM_DS, SM_DI,  MI_A, MS_A, SM_WL, M_L, IM_ADL, IM_WL, IM_DL, IM_AL,E}, ATOMICST) {
		setBlocked;	
		l_popAtomicRequestQueue;
	}

	transition({I, M, S, fIS_AD, fIS_D, IS_AD, IS_D, IS_DI, IM_AD, IM_D,  IM_DS, IM_DI, SM_AD,  SM_D, SM_DS, SM_DI,  MI_A, MS_A, SM_WL, M_L, IM_ADL, IM_WL, IM_DL, IM_AL,E}, ATOMICEN) {
		setUnblocked;
		l_popAtomicRequestQueue;
	}
}

